{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d91fac-c07b-461b-95bb-c51e8cd3b4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import tarfile\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel, AdamW, BertConfig\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "from tokenizers import ByteLevelBPETokenizer, BertWordPieceTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import os\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "summary = SummaryWriter()\n",
    "%load_ext tensorboard\n",
    "# tensorboard --logdir runs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "# Random_Seed 설정\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3023d145-3347-4bf4-897e-27b258731a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 불러오기\n",
    "\n",
    "# df1 = pd.read_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\device.csv\")\n",
    "# df2 = pd.read_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\logon.csv\")\n",
    "# df3 = pd.read_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\email.csv\")\n",
    "# df4 = pd.read_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\file.csv\")\n",
    "# df5 = pd.read_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\http.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8facea23-fb32-455d-8f87-24fc8887c673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 전처리1 date컬럼을 datetime 으로 변환, date컬럼을 인덱스로 변환\n",
    "\n",
    "# change_index_before_preprocess(df1)\n",
    "# change_index_before_preprocess(df2)\n",
    "# change_index_before_preprocess(df3)\n",
    "# change_index_before_preprocess(df4)\n",
    "# change_index_before_preprocess(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5ca4e9-0696-44d0-b74d-596f53e51e8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 전처리2 메일, 파일열람을 activity에 추가\n",
    "\n",
    "# df1['activity_bak'] = df1['activity']\n",
    "# df2['activity_bak'] = df2['activity']\n",
    "# df3['activity'] = 'Mail'\n",
    "# df3['activity_bak'] = df3['activity']\n",
    "# df4['activity'] = 'File'\n",
    "# df4['activity_bak'] = df4['activity']\n",
    "# df5['activity'] = 'Http'\n",
    "# df5['activity_bak'] = df5['activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613db357-1342-48eb-a74e-8905dbe367ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 전처리3 불러온 데이터를 하나의 테이블로 통합\n",
    "\n",
    "# con_df = pd.concat([df1,df2,df3,df4,df5],axis=0)\n",
    "# con_df.sort_values(by='date',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ccc334-4d0c-4b85-b76b-18f3a5890a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 전처리4 행동값(activity)을 치환\n",
    "\n",
    "# con_df['activity'] = con_df['activity'].map({'Logon':1,\n",
    "#                                              'Mail':2,\n",
    "#                                              'Http':3,\n",
    "#                                              'Connect':4,\n",
    "#                                              'Disconnect':5,\n",
    "#                                              'File':6,\n",
    "#                                              'Logoff':7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a6ce17-0f82-4d1e-8131-b6c12f6351dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 전처리4 행동값(activity)을 역으로 치환\n",
    "\n",
    "# con_df['activity'] = con_df['activity'].map({1:'Logon',\n",
    "#                                              2:'Mail',\n",
    "#                                              3:'Http',\n",
    "#                                              4:'Connect',\n",
    "#                                              5:'Disconnect',\n",
    "#                                              6:'File',\n",
    "#                                              7:'Logoff'})\n",
    "# con_df['date_bak'] = con_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce25e84-89c6-4a52-b74d-135ed5f0e307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 전처리된 데이터를 폴더에 저장\n",
    "\n",
    "# con_df = con_df[['user','pc','activity']]\n",
    "# con_df.to_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\preprocessed_user_pc_activity.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f34a168-60cf-4213-afb2-ceabe49f6a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tarfile 압축해제\n",
    "\n",
    "def unzip(url, name):\n",
    "    a = tarfile.open(url + \"\\\\\" + name + \".tar\")\n",
    "    a.extractall(url)\n",
    "    a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e426f4df-44e1-4f45-b5b6-cac26dc1f22c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 소프트맥스 함수\n",
    "\n",
    "def softmax(self):\n",
    "    exp_a = np.exp(self)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6501f7-a83c-4ebf-a2e2-1736eed209b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 불러오기\n",
    "# ex) attention_masks = load_pickle('attention_masks', tarfile_url, tarfile_name)\n",
    "\n",
    "def load_pickle(self, tarfile_url, tarfile_name):\n",
    "    with open(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\preprocessed_\" + self + \".pickle\",\"rb\") as fr:\n",
    "        self = pickle.load(fr)\n",
    "    return self\n",
    "\n",
    "# pickle 저장하기\n",
    "# ex) save_pickle(behavior, 'behavior', tarfile_url, tarfile_name)\n",
    "\n",
    "def save_pickle(self, name, tarfile_url, tarfile_name):\n",
    "    with open(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\preprocessed_\" + name +\".pickle\",\"wb\") as fw:\n",
    "        pickle.dump(self, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8f0d7a2-9247-478d-976f-b9de162811f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 컬럼을 datetime으로 변환하고 인덱스로 만들어서 반환하는 함수\n",
    "# ex) change_index_before_preprocess(df1)\n",
    "\n",
    "def change_index_before_preprocess(self):\n",
    "    if 'date' in self.columns:\n",
    "        self['date_bak'] = self['date']\n",
    "        self['date'] = pd.to_datetime(self['date'], format = \"%m/%d/%Y %H:%M:%S\")\n",
    "    if self.index.dtype == 'int64':\n",
    "        self.set_index('date',inplace=True)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ea9101-2d75-46e6-b824-cffd2f96e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간기반 행동치환(사용안함)\n",
    "# ex) behavior = behavior_convert_based_timen(con_df)\n",
    "\n",
    "def behavior_convert_based_time(con_df):\n",
    "    print('[+] START : make behavior tokens')\n",
    "    inside_start_index=0\n",
    "    behavior=[[]]\n",
    "    \n",
    "    for user_index in range(len(con_df['user'].unique())):\n",
    "        a = con_df.loc[(con_df['user']==con_df['user'].unique()[user_index])]\n",
    "        if inside_start_index==0:\n",
    "            behavior[inside_start_index].append(a.activity[0])\n",
    "\n",
    "        for x in range(1,len(a)):\n",
    "            if a.index[x-1].hour == a.index[x].hour:\n",
    "                behavior[inside_start_index].append(a.activity[x])\n",
    "\n",
    "            else:\n",
    "                inside_start_index += 1\n",
    "                behavior.append([a.activity[x]])\n",
    "    \n",
    "    return behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e10ab52f-b21f-46cf-8c65-ab1a675d71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세션기반 행동치환(텍스트로)\n",
    "# ex) behavior = behavior_convert_based_session(con_df)\n",
    "\n",
    "def behavior_convert_based_session(con_df):\n",
    "    print('[+] START : make behavior tokens')\n",
    "    behavior=[]\n",
    "    \n",
    "    \n",
    "    for user_index in range(len(con_df['user'].unique())):\n",
    "        a = con_df.loc[(con_df['user']==con_df['user'].unique()[user_index])]\n",
    "        sentence = \"\"\n",
    "        for x in range(len(a)):\n",
    "            if a['activity'][x]!=\"Logoff\":\n",
    "                if len(sentence)==0:\n",
    "                    sentence = a['activity'][x]\n",
    "                else:\n",
    "                    sentence = sentence + \" \" + str(a['activity'][x])\n",
    "            else:\n",
    "                sentence = sentence + \" \" + str(a['activity'][x])\n",
    "                behavior.append([sentence])\n",
    "                sentence = \"\"\n",
    "    \n",
    "    return behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e0750e4-f8bf-431a-a7ec-628a8f15b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답파일 전처리\n",
    "# ex) answer =  answer_preprocess(answer_url, tarfile_name)\n",
    "\n",
    "def answer_preprocess(answer_master_url, tarfile_name):\n",
    "    answer = pd.read_csv(answer_master_url)\n",
    "    change_object_to_datetime(answer,'start')\n",
    "    change_object_to_datetime(answer,'end')\n",
    "    answer = answer.loc[(answer['dataset']==float(tarfile_name[1:]))]\n",
    "    answer['index_bak'] = [x for x in range(len(answer))]\n",
    "    answer.set_index('index_bak', inplace=True)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "180b63f9-ea76-464b-b05e-af309653c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con_df 전처리\n",
    "# ex) con_df = con_df_preprocess(tarfile_name, tarfile_url)\n",
    "\n",
    "def con_df_preprocess(tarfile_name, tarfile_url):\n",
    "    con_df = pd.read_csv(tarfile_url + \"\\\\\" + tarfile_name + \"\\\\preprocessed_user_pc_activity.csv\", index_col='date')\n",
    "    con_df.index = pd.to_datetime(con_df.index)\n",
    "    # con_df\n",
    "    # con_df['activity'] = con_df['activity'].map({1:'Logon',\n",
    "    #                                              2:'Mail',\n",
    "    #                                              3:'Http',\n",
    "    #                                              4:'Connect',\n",
    "    #                                              5:'Disconnect',\n",
    "    #                                              6:'File',\n",
    "    #                                              7:'Logoff'})\n",
    "    con_df['date_bak'] = con_df.index\n",
    "    \n",
    "    return con_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cf3b98c-b573-43d5-a6f4-c9202ffb59e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 토큰만들기\n",
    "# min_slice = 15\n",
    "# token2 =make_token(min_slice)\n",
    "# token2\n",
    "def make_token(min_slice):\n",
    "    token={}\n",
    "    token['[PAD]']=0\n",
    "    token['[UNK]']=1\n",
    "    token['[CLS]']=2\n",
    "    token['[SEP]']=3\n",
    "    token['[MASK]']=4\n",
    "    \n",
    "    min_slicing = int(60/min_slice) \n",
    "    indexes = ['Logon','Http', 'Mail','Disconnect','Connect', 'File','Logoff']\n",
    "    start_num=5\n",
    "    for love in indexes:\n",
    "        for b in range(0,25):\n",
    "            if b <10:\n",
    "                b = str(0) + str(b)\n",
    "            if min_slice != 1:\n",
    "                for c in range(min_slicing):\n",
    "                    if min_slice*c < 10:\n",
    "                        imsi = str(0) + str(min_slice*c)\n",
    "                        token[love + '-' + str(b) + ':' + str(imsi)] = start_num\n",
    "                        start_num += 1\n",
    "                    else:\n",
    "                        token[love + '-' + str(b) + ':' + str(min_slice*c)] = start_num\n",
    "                        start_num += 1\n",
    "            else:\n",
    "                for c in range(min_slice):\n",
    "                    token[love + '-' + str(b) + ':00'] = start_num\n",
    "                    start_num += 1 \n",
    "    # print('startnum',start_num)\n",
    "    con_dfs = load_pickle('con_df', tarfile_url, tarfile_name)\n",
    "    pc_names = con_dfs['pc'].value_counts().index.to_list()\n",
    "    # print(pc_names)\n",
    "    for x in pc_names:\n",
    "        # print(x)\n",
    "        token[x] = start_num\n",
    "        start_num += 1\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a3b2ac0-2b90-4317-9232-f249a8301f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# date 컬럼을 datetime으로 변환하는 함수\n",
    "# ex) change_object_to_datetime(df1)\n",
    "\n",
    "def change_object_to_datetime(answer_df, column_name):\n",
    "    answer_df[column_name] = pd.to_datetime(answer_df[column_name], format = \"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb7713d0-0177-425f-8528-c7a180f22a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minute_slice(minute, min_slice):\n",
    "    minute=int(minute)+1\n",
    "    minute_list=[0]\n",
    "    if min_slice==60:\n",
    "        return '00'\n",
    "    else:\n",
    "        for x in range(1,int(60/min_slice)+1):\n",
    "            minute_list.append(min_slice*x)\n",
    "        # print(minute_list)\n",
    "        for x in range(len(minute_list)):\n",
    "            if minute <= minute_list[x]:\n",
    "                # if x==0:\n",
    "                #     if minute_list[0] < 10:\n",
    "                #         return '0' + str(minute_list[0])\n",
    "                #     else:\n",
    "                #         return str(minute_list[0])\n",
    "                # else:\n",
    "                if minute_list[x-1] < 10:\n",
    "                    return '0' + str(minute_list[x-1])\n",
    "                else:\n",
    "                    return str(minute_list[x-1])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e982459-0b23-46fc-83b0-3c76286ec932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 세션기반 행동치환(시간토큰)\n",
    "# ex) behavior = behavior_convert_based_session2(con_df,token)\n",
    "\n",
    "def behavior_convert_based_session2(con_df, answer ,token, scenario_class, answer_url, min_slice, loveuser):\n",
    "    # print('[+] START : make behavior tokens')\n",
    "    \n",
    "    \n",
    "    # 개별 answer 파일을 읽고 그안의 내용을 파싱함\n",
    "    all_answer=[]\n",
    "    pc = []\n",
    "    if loveuser in answer['user'].unique().tolist():\n",
    "        answer_file = glob.glob(answer_url + \"\\\\*\" + loveuser +\"*\")\n",
    "        with open(answer_file[0], \"r\") as tf:\n",
    "            line = tf.readline()\n",
    "            while line:\n",
    "                line = line.replace('\\n','')\n",
    "                lines = line.split(',')\n",
    "                if lines[0]=='http':\n",
    "                    lines[5]='Http'\n",
    "                elif lines[0]=='email':\n",
    "                    lines[5]='Mail'\n",
    "                elif lines[0]=='file':\n",
    "                    lines[5]='File'\n",
    "                cap_lines = lines[:6]\n",
    "                all_answer.append(cap_lines)\n",
    "                line = tf.readline()\n",
    "        # print(all_answer)\n",
    "        for y,x in enumerate(all_answer):\n",
    "            if all_answer[y][4] not in pc:\n",
    "                pc.append(all_answer[y][4])\n",
    "        \n",
    "    # print('pc : ',pc)\n",
    "    \n",
    "    normal_start_index = 0\n",
    "    mal_start_index = 0\n",
    "    behavior = [[]]\n",
    "    mal_user = answer['user'].unique().tolist()\n",
    "    mal_behavior = [[]]\n",
    "    con_df = con_df.loc[con_df.pc.isin(pc)]\n",
    "    \n",
    "    norm_https = []\n",
    "    mal_https = []\n",
    "    norm_http = 0\n",
    "    mal_http = 0\n",
    "    \n",
    "    norm_connects = []\n",
    "    mal_connects = []\n",
    "    norm_connect = 0\n",
    "    mal_connect = 0\n",
    "    \n",
    "    norm_mails = []\n",
    "    mal_mails = []\n",
    "    norm_mail = 0\n",
    "    mal_mail = 0\n",
    "    \n",
    "    for user_index in range(len(con_df['user'].unique())):\n",
    "        \n",
    "        for pc_index in range(len(con_df['pc'].unique())):\n",
    "            \n",
    "            # print(con_df['pc'].unique())\n",
    "            user_name = con_df['user'].unique()[user_index]\n",
    "            pc_name = con_df['pc'].unique()[pc_index]\n",
    "            \n",
    "            a = con_df.loc[(con_df['user']==user_name) & (con_df['pc']==pc_name)]\n",
    "            \n",
    "            if a['user'][0] in mal_user:\n",
    "                b, a = answer_timeline_seperate(user_name, answer, a, con_df, pc_name, scenario_class, answer_url)\n",
    "                \n",
    "                \n",
    "                # 비정상 행동 토크나이징\n",
    "                if len(b) >= 1:\n",
    "                    # if mal_start_index == 0:\n",
    "                    #     # mal_behavior[mal_start_index].append(2)\n",
    "                        # mal_behavior[mal_start_index].append(token[pc_name])\n",
    "                        # mal_behavior[mal_start_index].append(token.get(str((b['activity'][0]) + '-' + str(b['date_bak'][0])[11:13] + ':' + minute_slice(str(b['date_bak'][0])[14:16] ,min_slice))))\n",
    "                        \n",
    "                    for x in range(len(b)):                 \n",
    "                        if x ==0:\n",
    "                            mal_behavior[mal_start_index].append(token[pc_name])\n",
    "                            mal_behavior[mal_start_index].append(token.get(str((b['activity'][x]) + '-' + str(b['date_bak'][x])[11:13] + ':' + minute_slice(str(b['date_bak'][x])[14:16] ,min_slice))))\n",
    "                        else:\n",
    "                            if b['activity'][x] != \"Logoff\":                                \n",
    "                                if b['activity'][x] == \"Http\":\n",
    "                                    mal_http += 1\n",
    "                                if b['activity'][x] == \"Connect\":\n",
    "                                    mal_connect += 1\n",
    "                                if b['activity'][x] == \"Mail\":\n",
    "                                    mal_mail += 1\n",
    "                                mal_behavior[mal_start_index].append(token.get(str((b['activity'][x]) + '-' + str(b['date_bak'][x])[11:13] + ':' + minute_slice(str(b['date_bak'][x])[14:16] ,min_slice))))\n",
    "\n",
    "                            else:\n",
    "                                mal_behavior[mal_start_index].append(token.get(str((b['activity'][x]) + '-' + str(b['date_bak'][x])[11:13] + ':' + minute_slice(str(b['date_bak'][x])[14:16] ,min_slice))))\n",
    "                                # mal_behavior[mal_start_index].append(3)\n",
    "                                # print(mal_behavior[mal_start_index])\n",
    "                                mal_start_index += 1\n",
    "                                mal_behavior.append([])\n",
    "                                # mal_behavior[mal_start_index].append(2)\n",
    "                                if x != len(b)-1:\n",
    "                                    mal_behavior[mal_start_index].append(token[pc_name])\n",
    "                               \n",
    "                                \n",
    "                                mal_https.append(mal_http)\n",
    "                                mal_http = 0\n",
    "                                \n",
    "                                mal_connects.append(mal_connect)\n",
    "                                mal_connect = 0\n",
    "                                \n",
    "                                mal_mails.append(mal_mail)\n",
    "                                mal_mail = 0\n",
    "                                \n",
    "                    \n",
    "\n",
    "            \n",
    "            # 정상 행동 토크나이징\n",
    "            if len(a) >= 1:\n",
    "                # if normal_start_index == 0:\n",
    "                #     # behavior[normal_start_index].append(2)\n",
    "                #     # behavior[normal_start_index].append(token[pc_name])\n",
    "                #     behavior[normal_start_index].append(token.get(str((a['activity'][0]) + '-' + str(a['date_bak'][0])[11:13] + ':' + minute_slice(str(a['date_bak'][0])[14:16] ,min_slice))))\n",
    "\n",
    "                for x in range(len(a)):\n",
    "                    if x==0 :\n",
    "                        behavior[normal_start_index].append(token[pc_name])\n",
    "                        behavior[normal_start_index].append(token.get(str((a['activity'][x]) + '-' + str(a['date_bak'][x])[11:13] + ':' + minute_slice(str(a['date_bak'][x])[14:16] ,min_slice))))\n",
    "                    else:\n",
    "                        if a['activity'][x] != \"Logoff\":\n",
    "                            if a['activity'][x] == \"Http\":\n",
    "                                norm_http += 1\n",
    "                            if a['activity'][x] == \"Connect\":\n",
    "                                norm_connect += 1\n",
    "                            if a['activity'][x] == \"Mail\":\n",
    "                                norm_mail += 1\n",
    "                            behavior[normal_start_index].append(token.get(str((a['activity'][x]) + '-' + str(a['date_bak'][x])[11:13] + ':' + minute_slice(str(a['date_bak'][x])[14:16] ,min_slice))))\n",
    "\n",
    "                        else:\n",
    "                            behavior[normal_start_index].append(token.get(str((a['activity'][x]) + '-' + str(a['date_bak'][x])[11:13] + ':' + minute_slice(str(a['date_bak'][x])[14:16] ,min_slice))))\n",
    "                            # behavior[normal_start_index].append(3)\n",
    "\n",
    "                            normal_start_index += 1\n",
    "                            behavior.append([])\n",
    "                            # behavior[normal_start_index].append(2) \n",
    "                            if x != len(a)-1:\n",
    "                                behavior[normal_start_index].append(token[pc_name])\n",
    "                            \n",
    "                            norm_https.append(norm_http)\n",
    "                            norm_http = 0\n",
    "                            \n",
    "                            norm_connects.append(norm_connect)\n",
    "                            norm_connect = 0\n",
    "                            \n",
    "                            norm_mails.append(norm_mail)\n",
    "                            norm_mail = 0\n",
    "                            \n",
    "                           \n",
    "    \n",
    "    return behavior[:-1], mal_behavior[:-1], norm_https, mal_https, norm_connects, mal_connects, norm_mails, mal_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc8b0334-3882-445f-9544-123180968533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session token 생성할때 내부에서 answer time_line 분리시키는 함수\n",
    "# ex) b, a = answer_timeline_seperate(user_name , answer, a)\n",
    "\n",
    "def answer_timeline_seperate(attacker_name, answer, a, con_df, pc_name, scenario_class, answer_url):\n",
    "    \n",
    "    # 개별 answer 파일을 읽고 그안의 내용을 파싱함\n",
    "    all_answer=[]\n",
    "\n",
    "    answer_file = glob.glob(answer_url + \"\\\\*\" + attacker_name +\"*\")\n",
    "    with open(answer_file[0], \"r\") as tf:\n",
    "        line = tf.readline()\n",
    "        while line:\n",
    "            line = line.replace('\\n','')\n",
    "            lines = line.split(',')\n",
    "            if lines[0]=='http':\n",
    "                lines[5]='Http'\n",
    "            elif lines[0]=='email':\n",
    "                lines[5]='Mail'\n",
    "            elif lines[0]=='file':\n",
    "                lines[5]='File'\n",
    "            cap_lines = lines[:6]\n",
    "            if lines[4] == pc_name:\n",
    "                all_answer.append(cap_lines)\n",
    "            line = tf.readline()\n",
    "    scenario_class[int(answer.loc[answer.user==attacker_name].scenario)-1] = scenario_class[int(answer.loc[answer.user==attacker_name].scenario)-1] + len(all_answer)\n",
    "    # print(scenario_class)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 타임라인 추출(Mal_behavior가 나온 login~ logout 까지)\n",
    "    if len(all_answer) >= 1:\n",
    "        df = pd.DataFrame(all_answer)\n",
    "        \n",
    "        df.set_index(2,inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        time_lines=[[]]\n",
    "        start_index=0\n",
    "        \n",
    "        error_time_lines=[[]]\n",
    "        error_start_index=0\n",
    "        \n",
    "        # print(df[df.columns.difference([1])])\n",
    "\n",
    "        # df.index[x].date() = 2010-06-17\n",
    "        # df.index[x] = 2010-06-17 09:06:37\n",
    "\n",
    "        \n",
    "        for x in range(len(df)):\n",
    "            try:\n",
    "                session_start = a.loc[(a['activity']=='Logon') & (a['pc']==df[4][x])][:df.index[x]][-1:].index[0]\n",
    "            except:\n",
    "                print('')\n",
    "                # print(attacker_name, ' never login in ', pc_name, ' at ',df.index[x])\n",
    "                continue\n",
    "                # con_df = load_pickle('con_df', tarfile_url, tarfile_name)\n",
    "                # session_start = con_df.loc[(con_df.user==df[3][x]) & (con_df['activity']=='Logon') & (con_df['pc']==df[4][x])][:df.index[x]][-1:].index[0]\n",
    "\n",
    "            try:\n",
    "                session_end = a.loc[(a['activity']=='Logoff') & (a['pc']==df[4][x])][df.index[x]:][:1].index[0]\n",
    "            except:\n",
    "                # print(attacker_name, ' never logout in ', pc_name, ' at ',df.index[x])\n",
    "                print('')\n",
    "                continue\n",
    "                # con_df = load_pickle('con_df', tarfile_url, tarfile_name)\n",
    "                # session_end = con_df.loc[(con_df.user==df[3][x]) & (con_df['activity']=='Logoff') & (con_df['pc']==df[4][x])][df.index[x]:][:1].index[0]          \n",
    "\n",
    "            session_start = session_start.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            session_end = session_end.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            for z,y in enumerate(time_lines):\n",
    "                if session_start and session_end in y:\n",
    "                    break\n",
    "                elif z == len(time_lines) -1:\n",
    "                    time_lines[start_index].append(session_start)\n",
    "                    time_lines[start_index].append(session_end)\n",
    "                    time_lines.append([])\n",
    "                    start_index += 1\n",
    "                    break\n",
    "\n",
    "        time_lines = time_lines[:-1]\n",
    "        # print(time_lines)\n",
    "\n",
    "\n",
    "    # 추출된 타임라인 기반으로 정상데이터, 비정상데이터 슬라이싱\n",
    "    c=a.copy()\n",
    "    b=a[:1].copy()\n",
    "    if len(all_answer) >= 1:\n",
    "        for x in range(len(time_lines)):\n",
    "            start_date = time_lines[x][0]\n",
    "            end_date = time_lines[x][1]\n",
    "            normal_1 = c.loc[(c.index < start_date)]\n",
    "            normal_2 = c.loc[(c.index > end_date)]\n",
    "            normal_1 = normal_1.append(normal_2)\n",
    "\n",
    "            b = b.append(c[start_date:end_date])\n",
    "            c = normal_1\n",
    "    \n",
    "    return b[1:], c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25ebe57a-99a1-46c2-915a-cd62c9327bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# behavior token 생성\n",
    "# con_df의 Dataframe을 받아서 list 형식의 tensor로 치환해줌\n",
    "# ex) input_ids, attention_masks = make_behavior_token(con_df)\n",
    "\n",
    "def make_behavior_token(self):\n",
    "    # print('[+] START : make input_ids & attention_mask')\n",
    "    \n",
    "    behavior_token = [[]]\n",
    "    #behavior token 만듦\n",
    "    for x in range(len(self)):\n",
    "        for y in range(len(self.columns)):\n",
    "                behavior_token[x].append(self[y][x])\n",
    "        behavior_token.append([])\n",
    "\n",
    "    # return behavior_token\n",
    "    behavior_token = behavior_token[:-1]\n",
    "    \n",
    "    # #behavior token, attention_mask tensor생성\n",
    "    input_ids = torch.tensor(behavior_token)\n",
    "    input_ids = input_ids.to(torch.long)\n",
    "    attention_masks = np.where(input_ids != 0, 1, 0)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    attention_masks = attention_masks.to(torch.long)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d512a45-070b-46e0-a1d1-4c477620c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩된 input_ids를 받고 MaskedLM을 위한 Label, masked_input_ids를 생성하는 함수\n",
    "# ex) masked_input_ids, labels = create_masking(input_ids)\n",
    "\n",
    "def create_masking(self):\n",
    "    # print('[+] START : make masked_tokens_ids & labels')\n",
    "    \n",
    "    masked_input_ids = self.detach().clone()\n",
    "    \n",
    "    # MaskedLM을 위한 Label 생성, Mask와 원래단어의 매핑\n",
    "    labels = self.detach().clone()\n",
    "    \n",
    "    # create random array of floats with equal dimensions to input_ids tensor\n",
    "    rand = torch.rand(masked_input_ids.shape)\n",
    "    # create mask array\n",
    "    mask_arr = (rand < 0.15) * (masked_input_ids != 2) * \\\n",
    "               (masked_input_ids != 3) * (masked_input_ids != 0)\n",
    "    \n",
    "    selection = []\n",
    "\n",
    "    for i in range(masked_input_ids.shape[0]):\n",
    "        selection.append(\n",
    "            torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        )\n",
    "        \n",
    "    for i in range(masked_input_ids.shape[0]):\n",
    "        masked_input_ids[i, selection[i]] = 4\n",
    "\n",
    "    return masked_input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed900f8e-1123-4608-bf7d-d43c9966d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaskedLM Train용 커스텀 데이터셋 설정\n",
    "# ex) train_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "class Train_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, masked_input_ids, token_type_ids, attention_masks, labels):\n",
    "        self.masked_input_ids = masked_input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # masked_input_ids = self.masked_input_ids[idx]\n",
    "        # token_type_ids = self.token_type_ids[idx]\n",
    "        # attention_masks = self.attention_masks[idx]\n",
    "        # labels = self.labels[idx]        \n",
    "        return self.masked_input_ids[idx], self.token_type_ids[idx], self.attention_masks[idx], self.labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(masked_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56321602-bea1-4f1b-9a35-ee7855b3ab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaskedLM Validation용 커스텀 데이터셋 설정\n",
    "# ex) train_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "#     train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "class Val_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, val_masked_input_ids, val_token_type_ids, val_attention_masks, val_labels):\n",
    "        self.val_masked_input_ids = val_masked_input_ids\n",
    "        self.val_token_type_ids = val_token_type_ids\n",
    "        self.val_attention_masks = val_attention_masks\n",
    "        self.val_labels = val_labels\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # masked_input_ids = self.masked_input_ids[idx]\n",
    "        # token_type_ids = self.token_type_ids[idx]\n",
    "        # attention_masks = self.attention_masks[idx]\n",
    "        # labels = self.labels[idx]        \n",
    "        return self.val_masked_input_ids[idx], self.val_token_type_ids[idx], self.val_attention_masks[idx], self.val_labels[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(val_masked_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5009aa61-139a-427b-941f-d3f2b9036017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 추출용 커스텀 데이터셋 설정\n",
    "# ex) embedding_dataset = Embedding_Dataset(input_ids, attention_masks)\n",
    "#     embedding_loader = torch.utils.data.DataLoader(embedding_dataset, batch_size=64)\n",
    "\n",
    "class Embedding_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_masks):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_masks = self.attention_masks[idx]\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb01b692-19f5-4525-8f0d-5c4c03ba57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder Train용 커스텀 데이터셋 설정\n",
    "# ex) ae_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "#     ae_loader = torch.utils.data.DataLoader(ae_dataset, batch_size=1)\n",
    "\n",
    "class ae_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, all_features):\n",
    "        self.all_features = all_features\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # all_features = self.all_features[idx]\n",
    "        return self.all_features[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b4f548c-0893-4f2e-baf0-25c97954a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 : min_slice\n",
    "# 1 : tr_epochs)\n",
    "# 2 : avg_precision)\n",
    "# 3 ; avg_recall)\n",
    "# 4 : f1_score)\n",
    "# 5 : 정탐율\n",
    "# 6 : 오탐율\n",
    "# 7 : accuracy\n",
    "# 8 : weight\n",
    "\n",
    "\n",
    "def result_explotiation(score_list):\n",
    "    \n",
    "    score_list = score_list[:-1]\n",
    "    \n",
    "    # print(score_list)\n",
    "    best_f1 = 0\n",
    "    best_f1_index = 0\n",
    "\n",
    "    best_TP = 0\n",
    "    best_TP_index = 0\n",
    "\n",
    "    best_TN = 0\n",
    "    best_TN_index = 0\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_accuracy_index = 0\n",
    "    \n",
    "    best_scen1 = 0\n",
    "    best_scen1_index = 0\n",
    "    \n",
    "    best_scen2 = 0\n",
    "    best_scen2_index = 0\n",
    "    \n",
    "    best_scen3 = 0\n",
    "    best_scen3_index = 0\n",
    "    \n",
    "    best_scen4 = 0\n",
    "    best_scen4_index = 0\n",
    "    \n",
    "    best_scen5 = 0\n",
    "    best_scen5_index = 0\n",
    "    \n",
    "    for y, x in enumerate(score_list):\n",
    "        if x[4] > best_f1:\n",
    "                best_f1 = x[4]\n",
    "                best_f1_index = y \n",
    "        if x[9] > best_scen1:\n",
    "            best_scen1 = x[9]\n",
    "            best_scen1_index = y\n",
    "        if x[11] > best_scen2:\n",
    "            best_scen2 = x[11]\n",
    "            best_scen2_index = y \n",
    "        if x[13] > best_scen3:\n",
    "            best_scen3 = x[13]\n",
    "            best_scen3_index = y \n",
    "        if x[15] > best_scen4:\n",
    "            best_scen4 = x[15]\n",
    "            best_scen4_index = y \n",
    "        if x[17] > best_scen5:\n",
    "            best_scen5 = x[17]\n",
    "            best_scen5_index = y\n",
    "        # if x[5] > best_TP:\n",
    "        #     best_TP = x[5]\n",
    "        #     best_TP_index = y\n",
    "        # if x[6] > best_TN:\n",
    "        #     best_TN = x[6]\n",
    "        #     best_TN_index = y\n",
    "        # if x[7] > best_accuracy:\n",
    "        #     best_accuracy = x[7]\n",
    "        #     best_accuracy_index = y \n",
    "    \n",
    "    print('===============================best_f1 combination=========================')\n",
    "    print('min_slice : ',score_list[best_f1_index][0])\n",
    "    print('train_epoch : ',score_list[best_f1_index][1])\n",
    "    print('http weight : ',score_list[best_f1_index][8][0], '  connect weight : ',score_list[best_f1_index][8][1], '  mail weight : ',score_list[best_f1_index][8][2])\n",
    "    print('')\n",
    "    print(' 시나리오 1 정탐율 :',score_list[best_f1_index][19], ' -> ',score_list[best_f1_index][9])\n",
    "    print(' 시나리오 1 오탐율 :',score_list[best_f1_index][10])\n",
    "    print('')\n",
    "    print(' 시나리오 2 정탐율 :',score_list[best_f1_index][20], ' -> ',score_list[best_f1_index][11])\n",
    "    print(' 시나리오 2 오탐율 :',score_list[best_f1_index][12])\n",
    "    print('')\n",
    "    print(' 시나리오 3 정탐율 :',score_list[best_f1_index][21], ' -> ',score_list[best_f1_index][13])\n",
    "    print(' 시나리오 3 오탐율 :',score_list[best_f1_index][14])\n",
    "    print('')\n",
    "    print(' 시나리오 4 정탐율 :',score_list[best_f1_index][22], ' -> ',score_list[best_f1_index][15])\n",
    "    print(' 시나리오 4 오탐율 :',score_list[best_f1_index][16])\n",
    "    print('')\n",
    "    print(' 시나리오 5 정탐율 :',score_list[best_f1_index][23], ' -> ',score_list[best_f1_index][17])\n",
    "    print(' 시나리오 5 오탐율 :',score_list[best_f1_index][18])\n",
    "    print('')\n",
    "    print('전체 정탐율 : ',score_list[best_f1_index][5]) \n",
    "    print('전체 오탐율 : ',score_list[best_f1_index][6])\n",
    "    print('accuracy : ',score_list[best_f1_index][7])\n",
    "    print('precision : ',score_list[best_f1_index][2])\n",
    "    print('recall : ',score_list[best_f1_index][3])\n",
    "    print('f1 score : ',score_list[best_f1_index][4])\n",
    "    print('')\n",
    "    \n",
    "    print('=========================best_시나리오1 combination=========================')\n",
    "    print('min_slice : ',score_list[best_scen1_index][0])\n",
    "    print('train_epoch : ',score_list[best_scen1_index][1])\n",
    "    print('http weight : ',score_list[best_scen1_index][8][0], '  connect weight : ',score_list[best_scen1_index][8][1], '  mail weight : ',score_list[best_scen1_index][8][2])\n",
    "    print('')\n",
    "    print(' 시나리오 1 정탐율 :',score_list[best_scen1_index][19], ' -> ',score_list[best_scen1_index][9])\n",
    "    print(' 시나리오 1 오탐율 :',score_list[best_scen1_index][10])\n",
    "    print('')\n",
    "    print(' 시나리오 2 정탐율 :',score_list[best_scen1_index][20], ' -> ',score_list[best_scen1_index][11])\n",
    "    print(' 시나리오 2 오탐율 :',score_list[best_scen1_index][12])\n",
    "    print('')\n",
    "    print(' 시나리오 3 정탐율 :',score_list[best_scen1_index][21], ' -> ',score_list[best_scen1_index][13])\n",
    "    print(' 시나리오 3 오탐율 :',score_list[best_scen1_index][14])\n",
    "    print('')\n",
    "    print(' 시나리오 4 정탐율 :',score_list[best_scen1_index][22], ' -> ',score_list[best_scen1_index][15])\n",
    "    print(' 시나리오 4 오탐율 :',score_list[best_scen1_index][16])\n",
    "    print('')\n",
    "    print(' 시나리오 5 정탐율 :',score_list[best_scen1_index][23], ' -> ',score_list[best_scen1_index][17])\n",
    "    print(' 시나리오 5 오탐율 :',score_list[best_scen1_index][18])\n",
    "    print('')\n",
    "    print('전체 정탐율 : ',score_list[best_scen1_index][5]) \n",
    "    print('전체 오탐율 : ',score_list[best_scen1_index][6])\n",
    "    print('accuracy : ',score_list[best_scen1_index][7])\n",
    "    print('precision : ',score_list[best_scen1_index][2])\n",
    "    print('recall : ',score_list[best_scen1_index][3])\n",
    "    print('f1 score : ',score_list[best_scen1_index][4])\n",
    "    print('')\n",
    "    \n",
    "    print('=========================best_시나리오2 combination=========================')\n",
    "    print('min_slice : ',score_list[best_scen2_index][0])\n",
    "    print('train_epoch : ',score_list[best_scen2_index][1])\n",
    "    print('http weight : ',score_list[best_scen2_index][8][0], '  connect weight : ',score_list[best_scen2_index][8][1], '  mail weight : ',score_list[best_scen2_index][8][2])\n",
    "    print('')\n",
    "    print(' 시나리오 1 정탐율 :',score_list[best_scen2_index][19], ' -> ',score_list[best_scen2_index][9])\n",
    "    print(' 시나리오 1 오탐율 :',score_list[best_scen2_index][10])\n",
    "    print('')\n",
    "    print(' 시나리오 2 정탐율 :',score_list[best_scen2_index][20], ' -> ',score_list[best_scen2_index][11])\n",
    "    print(' 시나리오 2 오탐율 :',score_list[best_scen2_index][12])\n",
    "    print('')\n",
    "    print(' 시나리오 3 정탐율 :',score_list[best_scen2_index][21], ' -> ',score_list[best_scen2_index][13])\n",
    "    print(' 시나리오 3 오탐율 :',score_list[best_scen2_index][14])\n",
    "    print('')\n",
    "    print(' 시나리오 4 정탐율 :',score_list[best_scen2_index][22], ' -> ',score_list[best_scen2_index][15])\n",
    "    print(' 시나리오 4 오탐율 :',score_list[best_scen2_index][16])\n",
    "    print('')\n",
    "    print(' 시나리오 5 정탐율 :',score_list[best_scen2_index][23], ' -> ',score_list[best_scen2_index][17])\n",
    "    print(' 시나리오 5 오탐율 :',score_list[best_scen2_index][18])\n",
    "    print('')\n",
    "    print('전체 정탐율 : ',score_list[best_scen2_index][5]) \n",
    "    print('전체 오탐율 : ',score_list[best_scen2_index][6])\n",
    "    print('accuracy : ',score_list[best_scen2_index][7])\n",
    "    print('precision : ',score_list[best_scen2_index][2])\n",
    "    print('recall : ',score_list[best_scen2_index][3])\n",
    "    print('f1 score : ',score_list[best_scen2_index][4])\n",
    "    print('')\n",
    "    \n",
    "    print('=========================best_시나리오3 combination=========================')\n",
    "    print('min_slice : ',score_list[best_scen3_index][0])\n",
    "    print('train_epoch : ',score_list[best_scen3_index][1])\n",
    "    print('http weight : ',score_list[best_scen3_index][8][0], '  connect weight : ',score_list[best_scen3_index][8][1], '  mail weight : ',score_list[best_scen3_index][8][2])\n",
    "    print('')\n",
    "    print(' 시나리오 1 정탐율 :',score_list[best_scen3_index][19], ' -> ',score_list[best_scen3_index][9])\n",
    "    print(' 시나리오 1 오탐율 :',score_list[best_scen3_index][10])\n",
    "    print('')\n",
    "    print(' 시나리오 2 정탐율 :',score_list[best_scen3_index][20], ' -> ',score_list[best_scen3_index][11])\n",
    "    print(' 시나리오 2 오탐율 :',score_list[best_scen3_index][12])\n",
    "    print('')\n",
    "    print(' 시나리오 3 정탐율 :',score_list[best_scen3_index][21], ' -> ',score_list[best_scen3_index][13])\n",
    "    print(' 시나리오 3 오탐율 :',score_list[best_scen3_index][14])\n",
    "    print('')\n",
    "    print(' 시나리오 4 정탐율 :',score_list[best_scen3_index][22], ' -> ',score_list[best_scen3_index][15])\n",
    "    print(' 시나리오 4 오탐율 :',score_list[best_scen3_index][16])\n",
    "    print('')\n",
    "    print(' 시나리오 5 정탐율 :',score_list[best_scen3_index][23], ' -> ',score_list[best_scen3_index][17])\n",
    "    print(' 시나리오 5 오탐율 :',score_list[best_scen3_index][18])\n",
    "    print('')\n",
    "    print('전체 정탐율 : ',score_list[best_scen3_index][5]) \n",
    "    print('전체 오탐율 : ',score_list[best_scen3_index][6])\n",
    "    print('accuracy : ',score_list[best_scen3_index][7])\n",
    "    print('precision : ',score_list[best_scen3_index][2])\n",
    "    print('recall : ',score_list[best_scen3_index][3])\n",
    "    print('f1 score : ',score_list[best_scen3_index][4])\n",
    "    print('')\n",
    "    \n",
    "    print('=========================best_시나리오4 combination=========================')\n",
    "    print('min_slice : ',score_list[best_scen4_index][0])\n",
    "    print('train_epoch : ',score_list[best_scen4_index][1])\n",
    "    print('http weight : ',score_list[best_scen4_index][8][0], '  connect weight : ',score_list[best_scen4_index][8][1], '  mail weight : ',score_list[best_scen4_index][8][2])\n",
    "    print('')\n",
    "    print(' 시나리오 1 정탐율 :',score_list[best_scen4_index][19], ' -> ',score_list[best_scen4_index][9])\n",
    "    print(' 시나리오 1 오탐율 :',score_list[best_scen4_index][10])\n",
    "    print('')\n",
    "    print(' 시나리오 2 정탐율 :',score_list[best_scen4_index][20], ' -> ',score_list[best_scen4_index][11])\n",
    "    print(' 시나리오 2 오탐율 :',score_list[best_scen4_index][12])\n",
    "    print('')\n",
    "    print(' 시나리오 3 정탐율 :',score_list[best_scen4_index][21], ' -> ',score_list[best_scen4_index][13])\n",
    "    print(' 시나리오 3 오탐율 :',score_list[best_scen4_index][14])\n",
    "    print('')\n",
    "    print(' 시나리오 4 정탐율 :',score_list[best_scen4_index][22], ' -> ',score_list[best_scen4_index][15])\n",
    "    print(' 시나리오 4 오탐율 :',score_list[best_scen4_index][16])\n",
    "    print('')\n",
    "    print(' 시나리오 5 정탐율 :',score_list[best_scen4_index][23], ' -> ',score_list[best_scen4_index][17])\n",
    "    print(' 시나리오 5 오탐율 :',score_list[best_scen4_index][18])\n",
    "    print('')\n",
    "    print('전체 정탐율 : ',score_list[best_scen4_index][5]) \n",
    "    print('전체 오탐율 : ',score_list[best_scen4_index][6])\n",
    "    print('accuracy : ',score_list[best_scen4_index][7])\n",
    "    print('precision : ',score_list[best_scen4_index][2])\n",
    "    print('recall : ',score_list[best_scen4_index][3])\n",
    "    print('f1 score : ',score_list[best_scen4_index][4])\n",
    "    print('')\n",
    "    \n",
    "    print('=========================best_시나리오5 combination=========================')\n",
    "    print('min_slice : ',score_list[best_scen5_index][0])\n",
    "    print('train_epoch : ',score_list[best_scen5_index][1])\n",
    "    print('http weight : ',score_list[best_scen5_index][8][0], '  connect weight : ',score_list[best_scen5_index][8][1], '  mail weight : ',score_list[best_scen5_index][8][2])\n",
    "    print('')\n",
    "    print(' 시나리오 1 정탐율 :',score_list[best_scen5_index][19], ' -> ',score_list[best_scen5_index][9])\n",
    "    print(' 시나리오 1 오탐율 :',score_list[best_scen5_index][10])\n",
    "    print('')\n",
    "    print(' 시나리오 2 정탐율 :',score_list[best_scen5_index][20], ' -> ',score_list[best_scen5_index][11])\n",
    "    print(' 시나리오 2 오탐율 :',score_list[best_scen5_index][12])\n",
    "    print('')\n",
    "    print(' 시나리오 3 정탐율 :',score_list[best_scen5_index][21], ' -> ',score_list[best_scen5_index][13])\n",
    "    print(' 시나리오 3 오탐율 :',score_list[best_scen5_index][14])\n",
    "    print('')\n",
    "    print(' 시나리오 4 정탐율 :',score_list[best_scen5_index][22], ' -> ',score_list[best_scen5_index][15])\n",
    "    print(' 시나리오 4 오탐율 :',score_list[best_scen5_index][16])\n",
    "    print('')\n",
    "    print(' 시나리오 5 정탐율 :',score_list[best_scen5_index][23], ' -> ',score_list[best_scen5_index][17])\n",
    "    print(' 시나리오 5 오탐율 :',score_list[best_scen5_index][18])\n",
    "    print('')\n",
    "    print('전체 정탐율 : ',score_list[best_scen5_index][5]) \n",
    "    print('전체 오탐율 : ',score_list[best_scen5_index][6])\n",
    "    print('accuracy : ',score_list[best_scen5_index][7])\n",
    "    print('precision : ',score_list[best_scen5_index][2])\n",
    "    print('recall : ',score_list[best_scen5_index][3])\n",
    "    print('f1 score : ',score_list[best_scen5_index][4])\n",
    "    print('')\n",
    "    \n",
    "    # print('=========================result==========================')\n",
    "    # for y, x in enumerate(score_list):\n",
    "    #     print('min_slice : ', x[0]) \n",
    "    #     print('train_epochs : ', x[1])\n",
    "    #     print('http weight : ',x[8][0], '  connect weight : ',x[8][1], '  mail weight : ',x[8][2])\n",
    "    #     print('')\n",
    "    #     print(' 시나리오 1 정탐율 :',x[19], ' -> ',x[9],\"%\")\n",
    "    #     print(' 시나리오 1 오탐율 :',x[10],\"%\")\n",
    "    #     print('')\n",
    "    #     print(' 시나리오 2 정탐율 :',x[20], ' -> ',x[11],\"%\")\n",
    "    #     print(' 시나리오 2 오탐율 :',x[12],\"%\")\n",
    "    #     print('')\n",
    "    #     print(' 시나리오 3 정탐율 :',x[21], ' -> ',x[13],\"%\")\n",
    "    #     print(' 시나리오 3 오탐율 :',x[14],\"%\")\n",
    "    #     print('')\n",
    "    #     print(' 시나리오 4 정탐율 :',x[22], ' -> ',x[15],\"%\")\n",
    "    #     print(' 시나리오 4 오탐율 :',x[16],\"%\")\n",
    "    #     print('')\n",
    "    #     print(' 시나리오 5 정탐율 :',x[23], ' -> ',x[17],\"%\")\n",
    "    #     print(' 시나리오 5 오탐율 :',x[18],\"%\")\n",
    "    #     print('')\n",
    "    #     print('전체 정탐율 : ', x[5])\n",
    "    #     print('전체 오탐율 : ',score_list[best_f1_index][6])\n",
    "    #     print('accuracy : ', x[7])\n",
    "    #     print('avg_precision : ', x[2])\n",
    "    #     print('avg_recall : ', x[3])\n",
    "    #     print('f1_score : ', x[4])\n",
    "    #     print('')\n",
    "\n",
    "    # print('====best_정탐율 combination====')\n",
    "    # print('min_slice : ',score_list[best_TP_index][0])\n",
    "    # print('train_epoch : ',score_list[best_TP_index][1])\n",
    "    # print('weight : ',score_list[best_TP_index][8])\n",
    "    # print('')\n",
    "    # print('정탐율 : ',score_list[best_TP_index][5]) \n",
    "    # print('오탐율 : ',score_list[best_TP_index][6])\n",
    "    # print('accuracy : ',score_list[best_TP_index][7])\n",
    "    # print('precision : ',score_list[best_TP_index][2])\n",
    "    # print('recall : ',score_list[best_TP_index][3])\n",
    "    # print('f1 score : ',score_list[best_TP_index][4])\n",
    "    # print('')\n",
    "    # print('====best_오탐율 combination====')\n",
    "    # print('min_slice : ',score_list[best_TN_index][0])\n",
    "    # print('train_epoch : ',score_list[best_TN_index][1])\n",
    "    # print('weight : ',score_list[best_TN_index][8])\n",
    "    # print('')\n",
    "    # print('정탐율 : ',score_list[best_TN_index][5]) \n",
    "    # print('오탐율 : ',score_list[best_TN_index][6])\n",
    "    # print('accuracy : ',score_list[best_TN_index][7])\n",
    "    # print('precision : ',score_list[best_TN_index][2])\n",
    "    # print('recall : ',score_list[best_TN_index][3])\n",
    "    # print('f1 score : ',score_list[best_TN_index][4])\n",
    "    # print('')\n",
    "    # print('====best_accuracy combination====')\n",
    "    # print('min_slice : ',score_list[best_accuracy_index][0])\n",
    "    # print('train_epoch : ',score_list[best_accuracy_index][1])\n",
    "    # print('weight : ',score_list[best_accuracy_index][8])\n",
    "    # print('')\n",
    "    # print('정탐율 : ',score_list[best_accuracy_index][5]) \n",
    "    # print('오탐율 : ',score_list[best_accuracy_index][6])\n",
    "    # print('accuracy : ',score_list[best_accuracy_index][7])\n",
    "    # print('precision : ',score_list[best_accuracy_index][2])\n",
    "    # print('recall : ',score_list[best_accuracy_index][3])\n",
    "    # print('f1 score : ',score_list[best_accuracy_index][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a3e19f6-1b72-4e82-88b5-676438b633cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_standardize(list1, list2,weight):\n",
    "    imsi_list = list1.copy()\n",
    "    for x in list2:\n",
    "        imsi_list.append(x)\n",
    "    \n",
    "    mean = np.mean(imsi_list)\n",
    "    variance = round(np.var(imsi_list),4)\n",
    "    std_variance = round(math.sqrt(variance),4)\n",
    "    \n",
    "    imsi1 = []\n",
    "    imsi2 = []\n",
    "    for x in list1:\n",
    "        if (x-mean)/std_variance > 0:\n",
    "            imsi1.append(((x-mean)/std_variance)*weight)\n",
    "        else:\n",
    "            imsi1.append(0)\n",
    "        \n",
    "    for x in list2:\n",
    "        if (x-mean)/std_variance > 0:\n",
    "            imsi2.append(((x-mean)/std_variance)*weight)\n",
    "        else:\n",
    "            imsi2.append(0)\n",
    "    return imsi1, imsi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f025fab1-b5d2-4b13-9cd2-16b20070ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 디코딩\n",
    "# ex) decode_token(mal_behavior[0],token)\n",
    "\n",
    "def decode_token(tokenized, token):\n",
    "    a=\"\"\n",
    "    for x in tokenized:\n",
    "        for key, value in token.items():\n",
    "            if x == value:\n",
    "                a = a + ' ' +str(key)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e00b6a8-b56d-4576-ae4c-236721ae6abe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>scenario</th>\n",
       "      <th>details</th>\n",
       "      <th>user</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_bak</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-AAM0658.csv</td>\n",
       "      <td>AAM0658</td>\n",
       "      <td>2010-10-23 01:34:19</td>\n",
       "      <td>2010-10-29 05:23:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-AJR0932.csv</td>\n",
       "      <td>AJR0932</td>\n",
       "      <td>2010-09-10 19:12:01</td>\n",
       "      <td>2010-09-18 02:02:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-BDV0168.csv</td>\n",
       "      <td>BDV0168</td>\n",
       "      <td>2010-07-30 19:56:44</td>\n",
       "      <td>2010-08-10 05:16:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-BIH0745.csv</td>\n",
       "      <td>BIH0745</td>\n",
       "      <td>2010-07-13 20:15:23</td>\n",
       "      <td>2010-07-13 21:20:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-BLS0678.csv</td>\n",
       "      <td>BLS0678</td>\n",
       "      <td>2010-09-21 01:16:22</td>\n",
       "      <td>2010-09-30 04:48:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-BTL0226.csv</td>\n",
       "      <td>BTL0226</td>\n",
       "      <td>2010-10-06 22:25:52</td>\n",
       "      <td>2010-10-14 06:43:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-CAH0936.csv</td>\n",
       "      <td>CAH0936</td>\n",
       "      <td>2010-08-11 04:00:08</td>\n",
       "      <td>2010-08-12 23:56:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-DCH0843.csv</td>\n",
       "      <td>DCH0843</td>\n",
       "      <td>2011-02-04 07:08:00</td>\n",
       "      <td>2011-02-04 07:36:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-EHB0824.csv</td>\n",
       "      <td>EHB0824</td>\n",
       "      <td>2010-07-22 21:48:43</td>\n",
       "      <td>2010-07-29 01:08:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-EHD0584.csv</td>\n",
       "      <td>EHD0584</td>\n",
       "      <td>2010-10-02 03:46:16</td>\n",
       "      <td>2010-10-08 22:26:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-FMG0527.csv</td>\n",
       "      <td>FMG0527</td>\n",
       "      <td>2011-01-05 21:53:29</td>\n",
       "      <td>2011-01-12 01:15:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-FTM0406.csv</td>\n",
       "      <td>FTM0406</td>\n",
       "      <td>2010-11-25 06:35:11</td>\n",
       "      <td>2010-12-02 00:35:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-GHL0460.csv</td>\n",
       "      <td>GHL0460</td>\n",
       "      <td>2010-11-09 06:28:40</td>\n",
       "      <td>2010-11-09 07:08:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-HJB0742.csv</td>\n",
       "      <td>HJB0742</td>\n",
       "      <td>2010-11-19 05:53:16</td>\n",
       "      <td>2010-11-25 04:51:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-JMB0308.csv</td>\n",
       "      <td>JMB0308</td>\n",
       "      <td>2010-07-14 00:56:15</td>\n",
       "      <td>2010-07-21 01:46:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-JRG0207.csv</td>\n",
       "      <td>JRG0207</td>\n",
       "      <td>2011-01-19 20:25:05</td>\n",
       "      <td>2011-01-26 02:38:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-KLH0596.csv</td>\n",
       "      <td>KLH0596</td>\n",
       "      <td>2011-02-12 07:11:50</td>\n",
       "      <td>2011-02-12 07:33:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-KPC0073.csv</td>\n",
       "      <td>KPC0073</td>\n",
       "      <td>2010-07-07 20:05:29</td>\n",
       "      <td>2010-07-15 03:08:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-LJR0523.csv</td>\n",
       "      <td>LJR0523</td>\n",
       "      <td>2010-07-31 07:43:15</td>\n",
       "      <td>2010-08-11 04:28:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-LQC0479.csv</td>\n",
       "      <td>LQC0479</td>\n",
       "      <td>2010-09-14 19:46:17</td>\n",
       "      <td>2010-09-22 01:08:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-MAR0955.csv</td>\n",
       "      <td>MAR0955</td>\n",
       "      <td>2011-02-08 05:55:53</td>\n",
       "      <td>2011-02-11 06:29:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-MAS0025.csv</td>\n",
       "      <td>MAS0025</td>\n",
       "      <td>2010-09-29 01:39:20</td>\n",
       "      <td>2010-09-30 22:39:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-MCF0600.csv</td>\n",
       "      <td>MCF0600</td>\n",
       "      <td>2010-09-20 23:52:19</td>\n",
       "      <td>2010-09-23 02:00:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-MYD0978.csv</td>\n",
       "      <td>MYD0978</td>\n",
       "      <td>2010-12-13 20:30:07</td>\n",
       "      <td>2010-12-18 07:02:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-PPF0435.csv</td>\n",
       "      <td>PPF0435</td>\n",
       "      <td>2011-02-09 03:00:27</td>\n",
       "      <td>2011-02-09 06:46:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-RAB0589.csv</td>\n",
       "      <td>RAB0589</td>\n",
       "      <td>2010-09-13 23:18:53</td>\n",
       "      <td>2010-09-23 07:30:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-RGG0064.csv</td>\n",
       "      <td>RGG0064</td>\n",
       "      <td>2010-10-20 20:12:23</td>\n",
       "      <td>2010-10-27 03:03:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-RKD0604.csv</td>\n",
       "      <td>RKD0604</td>\n",
       "      <td>2010-07-13 20:04:53</td>\n",
       "      <td>2010-07-20 03:36:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-TAP0551.csv</td>\n",
       "      <td>TAP0551</td>\n",
       "      <td>2010-10-23 02:55:51</td>\n",
       "      <td>2010-10-29 05:56:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.2</td>\n",
       "      <td>1</td>\n",
       "      <td>r4.2-1-WDD0366.csv</td>\n",
       "      <td>WDD0366</td>\n",
       "      <td>2011-02-24 19:49:41</td>\n",
       "      <td>2011-03-03 01:01:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-AAF0535.csv</td>\n",
       "      <td>AAF0535</td>\n",
       "      <td>2010-06-28 08:51:08</td>\n",
       "      <td>2010-08-20 15:38:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-ABC0174.csv</td>\n",
       "      <td>ABC0174</td>\n",
       "      <td>2010-10-27 14:11:03</td>\n",
       "      <td>2010-12-24 15:53:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-AKR0057.csv</td>\n",
       "      <td>AKR0057</td>\n",
       "      <td>2010-10-04 08:48:26</td>\n",
       "      <td>2010-11-29 18:48:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-CCL0068.csv</td>\n",
       "      <td>CCL0068</td>\n",
       "      <td>2010-12-27 09:20:16</td>\n",
       "      <td>2011-02-21 08:25:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-CEJ0109.csv</td>\n",
       "      <td>CEJ0109</td>\n",
       "      <td>2011-02-07 11:16:58</td>\n",
       "      <td>2011-04-01 17:23:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-CQW0652.csv</td>\n",
       "      <td>CQW0652</td>\n",
       "      <td>2011-02-18 09:22:22</td>\n",
       "      <td>2011-04-14 20:58:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-DIB0285.csv</td>\n",
       "      <td>DIB0285</td>\n",
       "      <td>2010-07-26 16:23:45</td>\n",
       "      <td>2010-09-13 17:13:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-DRR0162.csv</td>\n",
       "      <td>DRR0162</td>\n",
       "      <td>2010-11-11 07:01:17</td>\n",
       "      <td>2011-01-04 15:43:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-EDB0714.csv</td>\n",
       "      <td>EDB0714</td>\n",
       "      <td>2010-10-18 10:58:12</td>\n",
       "      <td>2010-12-14 14:17:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-EGD0132.csv</td>\n",
       "      <td>EGD0132</td>\n",
       "      <td>2010-08-02 10:01:20</td>\n",
       "      <td>2010-09-28 17:24:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-FSC0601.csv</td>\n",
       "      <td>FSC0601</td>\n",
       "      <td>2011-01-18 08:42:08</td>\n",
       "      <td>2011-03-17 16:45:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-HBO0413.csv</td>\n",
       "      <td>HBO0413</td>\n",
       "      <td>2011-02-14 08:15:24</td>\n",
       "      <td>2011-04-08 15:25:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-HXL0968.csv</td>\n",
       "      <td>HXL0968</td>\n",
       "      <td>2010-08-31 07:51:41</td>\n",
       "      <td>2010-10-28 16:06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-IJM0776.csv</td>\n",
       "      <td>IJM0776</td>\n",
       "      <td>2010-07-06 08:33:06</td>\n",
       "      <td>2010-09-01 14:10:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-IKR0401.csv</td>\n",
       "      <td>IKR0401</td>\n",
       "      <td>2010-12-27 08:57:14</td>\n",
       "      <td>2011-02-17 18:21:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-IUB0565.csv</td>\n",
       "      <td>IUB0565</td>\n",
       "      <td>2010-10-06 09:20:43</td>\n",
       "      <td>2010-11-30 17:49:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-JJM0203.csv</td>\n",
       "      <td>JJM0203</td>\n",
       "      <td>2010-09-02 07:40:34</td>\n",
       "      <td>2010-10-19 17:04:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-KRL0501.csv</td>\n",
       "      <td>KRL0501</td>\n",
       "      <td>2010-11-22 09:38:30</td>\n",
       "      <td>2011-01-19 17:06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-LCC0819.csv</td>\n",
       "      <td>LCC0819</td>\n",
       "      <td>2010-06-16 11:39:58</td>\n",
       "      <td>2010-08-10 18:31:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-MDH0580.csv</td>\n",
       "      <td>MDH0580</td>\n",
       "      <td>2011-01-04 09:13:35</td>\n",
       "      <td>2011-03-03 11:40:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-MOS0047.csv</td>\n",
       "      <td>MOS0047</td>\n",
       "      <td>2010-07-15 07:23:37</td>\n",
       "      <td>2010-09-10 19:34:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-NWT0098.csv</td>\n",
       "      <td>NWT0098</td>\n",
       "      <td>2011-02-07 16:40:17</td>\n",
       "      <td>2011-04-05 14:00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-PNL0301.csv</td>\n",
       "      <td>PNL0301</td>\n",
       "      <td>2010-06-14 09:25:31</td>\n",
       "      <td>2010-08-03 16:54:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-PSF0133.csv</td>\n",
       "      <td>PSF0133</td>\n",
       "      <td>2010-08-02 10:46:58</td>\n",
       "      <td>2010-09-29 16:23:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-RAR0725.csv</td>\n",
       "      <td>RAR0725</td>\n",
       "      <td>2010-07-06 08:07:33</td>\n",
       "      <td>2010-08-19 09:58:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-RHL0992.csv</td>\n",
       "      <td>RHL0992</td>\n",
       "      <td>2010-07-13 09:46:08</td>\n",
       "      <td>2010-09-09 20:45:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-RMW0542.csv</td>\n",
       "      <td>RMW0542</td>\n",
       "      <td>2010-06-21 09:04:43</td>\n",
       "      <td>2010-08-18 17:37:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-TNM0961.csv</td>\n",
       "      <td>TNM0961</td>\n",
       "      <td>2010-10-15 07:04:48</td>\n",
       "      <td>2010-12-09 16:05:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-VSS0154.csv</td>\n",
       "      <td>VSS0154</td>\n",
       "      <td>2010-09-07 09:49:28</td>\n",
       "      <td>2010-10-29 16:40:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.2</td>\n",
       "      <td>2</td>\n",
       "      <td>r4.2-2-XHW0498.csv</td>\n",
       "      <td>XHW0498</td>\n",
       "      <td>2010-08-09 12:48:49</td>\n",
       "      <td>2010-10-06 16:05:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-BBS0039.csv</td>\n",
       "      <td>BBS0039</td>\n",
       "      <td>2010-08-12 10:24:05</td>\n",
       "      <td>2010-08-13 19:08:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-BSS0369.csv</td>\n",
       "      <td>BSS0369</td>\n",
       "      <td>2010-09-30 13:31:56</td>\n",
       "      <td>2010-10-01 19:48:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-CCA0046.csv</td>\n",
       "      <td>CCA0046</td>\n",
       "      <td>2010-10-14 08:23:18</td>\n",
       "      <td>2010-10-15 20:01:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-CSC0217.csv</td>\n",
       "      <td>CSC0217</td>\n",
       "      <td>2010-06-10 07:54:10</td>\n",
       "      <td>2010-06-11 17:42:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-GTD0219.csv</td>\n",
       "      <td>GTD0219</td>\n",
       "      <td>2010-06-17 09:06:37</td>\n",
       "      <td>2010-06-18 17:50:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-JGT0221.csv</td>\n",
       "      <td>JGT0221</td>\n",
       "      <td>2010-07-15 09:43:23</td>\n",
       "      <td>2010-07-16 19:33:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-JLM0364.csv</td>\n",
       "      <td>JLM0364</td>\n",
       "      <td>2011-04-28 09:50:07</td>\n",
       "      <td>2011-04-29 20:04:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-JTM0223.csv</td>\n",
       "      <td>JTM0223</td>\n",
       "      <td>2010-07-22 07:27:46</td>\n",
       "      <td>2010-07-23 18:01:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-MPM0220.csv</td>\n",
       "      <td>MPM0220</td>\n",
       "      <td>2010-11-04 08:59:55</td>\n",
       "      <td>2010-11-05 19:29:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4.2</td>\n",
       "      <td>3</td>\n",
       "      <td>r4.2-3-MSO0222.csv</td>\n",
       "      <td>MSO0222</td>\n",
       "      <td>2010-12-09 08:36:36</td>\n",
       "      <td>2010-12-10 17:39:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset  scenario             details     user               start  \\\n",
       "index_bak                                                                       \n",
       "0              4.2         1  r4.2-1-AAM0658.csv  AAM0658 2010-10-23 01:34:19   \n",
       "1              4.2         1  r4.2-1-AJR0932.csv  AJR0932 2010-09-10 19:12:01   \n",
       "2              4.2         1  r4.2-1-BDV0168.csv  BDV0168 2010-07-30 19:56:44   \n",
       "3              4.2         1  r4.2-1-BIH0745.csv  BIH0745 2010-07-13 20:15:23   \n",
       "4              4.2         1  r4.2-1-BLS0678.csv  BLS0678 2010-09-21 01:16:22   \n",
       "5              4.2         1  r4.2-1-BTL0226.csv  BTL0226 2010-10-06 22:25:52   \n",
       "6              4.2         1  r4.2-1-CAH0936.csv  CAH0936 2010-08-11 04:00:08   \n",
       "7              4.2         1  r4.2-1-DCH0843.csv  DCH0843 2011-02-04 07:08:00   \n",
       "8              4.2         1  r4.2-1-EHB0824.csv  EHB0824 2010-07-22 21:48:43   \n",
       "9              4.2         1  r4.2-1-EHD0584.csv  EHD0584 2010-10-02 03:46:16   \n",
       "10             4.2         1  r4.2-1-FMG0527.csv  FMG0527 2011-01-05 21:53:29   \n",
       "11             4.2         1  r4.2-1-FTM0406.csv  FTM0406 2010-11-25 06:35:11   \n",
       "12             4.2         1  r4.2-1-GHL0460.csv  GHL0460 2010-11-09 06:28:40   \n",
       "13             4.2         1  r4.2-1-HJB0742.csv  HJB0742 2010-11-19 05:53:16   \n",
       "14             4.2         1  r4.2-1-JMB0308.csv  JMB0308 2010-07-14 00:56:15   \n",
       "15             4.2         1  r4.2-1-JRG0207.csv  JRG0207 2011-01-19 20:25:05   \n",
       "16             4.2         1  r4.2-1-KLH0596.csv  KLH0596 2011-02-12 07:11:50   \n",
       "17             4.2         1  r4.2-1-KPC0073.csv  KPC0073 2010-07-07 20:05:29   \n",
       "18             4.2         1  r4.2-1-LJR0523.csv  LJR0523 2010-07-31 07:43:15   \n",
       "19             4.2         1  r4.2-1-LQC0479.csv  LQC0479 2010-09-14 19:46:17   \n",
       "20             4.2         1  r4.2-1-MAR0955.csv  MAR0955 2011-02-08 05:55:53   \n",
       "21             4.2         1  r4.2-1-MAS0025.csv  MAS0025 2010-09-29 01:39:20   \n",
       "22             4.2         1  r4.2-1-MCF0600.csv  MCF0600 2010-09-20 23:52:19   \n",
       "23             4.2         1  r4.2-1-MYD0978.csv  MYD0978 2010-12-13 20:30:07   \n",
       "24             4.2         1  r4.2-1-PPF0435.csv  PPF0435 2011-02-09 03:00:27   \n",
       "25             4.2         1  r4.2-1-RAB0589.csv  RAB0589 2010-09-13 23:18:53   \n",
       "26             4.2         1  r4.2-1-RGG0064.csv  RGG0064 2010-10-20 20:12:23   \n",
       "27             4.2         1  r4.2-1-RKD0604.csv  RKD0604 2010-07-13 20:04:53   \n",
       "28             4.2         1  r4.2-1-TAP0551.csv  TAP0551 2010-10-23 02:55:51   \n",
       "29             4.2         1  r4.2-1-WDD0366.csv  WDD0366 2011-02-24 19:49:41   \n",
       "30             4.2         2  r4.2-2-AAF0535.csv  AAF0535 2010-06-28 08:51:08   \n",
       "31             4.2         2  r4.2-2-ABC0174.csv  ABC0174 2010-10-27 14:11:03   \n",
       "32             4.2         2  r4.2-2-AKR0057.csv  AKR0057 2010-10-04 08:48:26   \n",
       "33             4.2         2  r4.2-2-CCL0068.csv  CCL0068 2010-12-27 09:20:16   \n",
       "34             4.2         2  r4.2-2-CEJ0109.csv  CEJ0109 2011-02-07 11:16:58   \n",
       "35             4.2         2  r4.2-2-CQW0652.csv  CQW0652 2011-02-18 09:22:22   \n",
       "36             4.2         2  r4.2-2-DIB0285.csv  DIB0285 2010-07-26 16:23:45   \n",
       "37             4.2         2  r4.2-2-DRR0162.csv  DRR0162 2010-11-11 07:01:17   \n",
       "38             4.2         2  r4.2-2-EDB0714.csv  EDB0714 2010-10-18 10:58:12   \n",
       "39             4.2         2  r4.2-2-EGD0132.csv  EGD0132 2010-08-02 10:01:20   \n",
       "40             4.2         2  r4.2-2-FSC0601.csv  FSC0601 2011-01-18 08:42:08   \n",
       "41             4.2         2  r4.2-2-HBO0413.csv  HBO0413 2011-02-14 08:15:24   \n",
       "42             4.2         2  r4.2-2-HXL0968.csv  HXL0968 2010-08-31 07:51:41   \n",
       "43             4.2         2  r4.2-2-IJM0776.csv  IJM0776 2010-07-06 08:33:06   \n",
       "44             4.2         2  r4.2-2-IKR0401.csv  IKR0401 2010-12-27 08:57:14   \n",
       "45             4.2         2  r4.2-2-IUB0565.csv  IUB0565 2010-10-06 09:20:43   \n",
       "46             4.2         2  r4.2-2-JJM0203.csv  JJM0203 2010-09-02 07:40:34   \n",
       "47             4.2         2  r4.2-2-KRL0501.csv  KRL0501 2010-11-22 09:38:30   \n",
       "48             4.2         2  r4.2-2-LCC0819.csv  LCC0819 2010-06-16 11:39:58   \n",
       "49             4.2         2  r4.2-2-MDH0580.csv  MDH0580 2011-01-04 09:13:35   \n",
       "50             4.2         2  r4.2-2-MOS0047.csv  MOS0047 2010-07-15 07:23:37   \n",
       "51             4.2         2  r4.2-2-NWT0098.csv  NWT0098 2011-02-07 16:40:17   \n",
       "52             4.2         2  r4.2-2-PNL0301.csv  PNL0301 2010-06-14 09:25:31   \n",
       "53             4.2         2  r4.2-2-PSF0133.csv  PSF0133 2010-08-02 10:46:58   \n",
       "54             4.2         2  r4.2-2-RAR0725.csv  RAR0725 2010-07-06 08:07:33   \n",
       "55             4.2         2  r4.2-2-RHL0992.csv  RHL0992 2010-07-13 09:46:08   \n",
       "56             4.2         2  r4.2-2-RMW0542.csv  RMW0542 2010-06-21 09:04:43   \n",
       "57             4.2         2  r4.2-2-TNM0961.csv  TNM0961 2010-10-15 07:04:48   \n",
       "58             4.2         2  r4.2-2-VSS0154.csv  VSS0154 2010-09-07 09:49:28   \n",
       "59             4.2         2  r4.2-2-XHW0498.csv  XHW0498 2010-08-09 12:48:49   \n",
       "60             4.2         3  r4.2-3-BBS0039.csv  BBS0039 2010-08-12 10:24:05   \n",
       "61             4.2         3  r4.2-3-BSS0369.csv  BSS0369 2010-09-30 13:31:56   \n",
       "62             4.2         3  r4.2-3-CCA0046.csv  CCA0046 2010-10-14 08:23:18   \n",
       "63             4.2         3  r4.2-3-CSC0217.csv  CSC0217 2010-06-10 07:54:10   \n",
       "64             4.2         3  r4.2-3-GTD0219.csv  GTD0219 2010-06-17 09:06:37   \n",
       "65             4.2         3  r4.2-3-JGT0221.csv  JGT0221 2010-07-15 09:43:23   \n",
       "66             4.2         3  r4.2-3-JLM0364.csv  JLM0364 2011-04-28 09:50:07   \n",
       "67             4.2         3  r4.2-3-JTM0223.csv  JTM0223 2010-07-22 07:27:46   \n",
       "68             4.2         3  r4.2-3-MPM0220.csv  MPM0220 2010-11-04 08:59:55   \n",
       "69             4.2         3  r4.2-3-MSO0222.csv  MSO0222 2010-12-09 08:36:36   \n",
       "\n",
       "                          end  \n",
       "index_bak                      \n",
       "0         2010-10-29 05:23:28  \n",
       "1         2010-09-18 02:02:51  \n",
       "2         2010-08-10 05:16:41  \n",
       "3         2010-07-13 21:20:44  \n",
       "4         2010-09-30 04:48:19  \n",
       "5         2010-10-14 06:43:29  \n",
       "6         2010-08-12 23:56:19  \n",
       "7         2011-02-04 07:36:05  \n",
       "8         2010-07-29 01:08:41  \n",
       "9         2010-10-08 22:26:26  \n",
       "10        2011-01-12 01:15:35  \n",
       "11        2010-12-02 00:35:19  \n",
       "12        2010-11-09 07:08:45  \n",
       "13        2010-11-25 04:51:16  \n",
       "14        2010-07-21 01:46:49  \n",
       "15        2011-01-26 02:38:27  \n",
       "16        2011-02-12 07:33:24  \n",
       "17        2010-07-15 03:08:16  \n",
       "18        2010-08-11 04:28:49  \n",
       "19        2010-09-22 01:08:02  \n",
       "20        2011-02-11 06:29:08  \n",
       "21        2010-09-30 22:39:20  \n",
       "22        2010-09-23 02:00:46  \n",
       "23        2010-12-18 07:02:34  \n",
       "24        2011-02-09 06:46:27  \n",
       "25        2010-09-23 07:30:03  \n",
       "26        2010-10-27 03:03:52  \n",
       "27        2010-07-20 03:36:55  \n",
       "28        2010-10-29 05:56:18  \n",
       "29        2011-03-03 01:01:02  \n",
       "30        2010-08-20 15:38:04  \n",
       "31        2010-12-24 15:53:02  \n",
       "32        2010-11-29 18:48:10  \n",
       "33        2011-02-21 08:25:17  \n",
       "34        2011-04-01 17:23:43  \n",
       "35        2011-04-14 20:58:56  \n",
       "36        2010-09-13 17:13:29  \n",
       "37        2011-01-04 15:43:53  \n",
       "38        2010-12-14 14:17:02  \n",
       "39        2010-09-28 17:24:42  \n",
       "40        2011-03-17 16:45:04  \n",
       "41        2011-04-08 15:25:10  \n",
       "42        2010-10-28 16:06:01  \n",
       "43        2010-09-01 14:10:31  \n",
       "44        2011-02-17 18:21:57  \n",
       "45        2010-11-30 17:49:20  \n",
       "46        2010-10-19 17:04:21  \n",
       "47        2011-01-19 17:06:28  \n",
       "48        2010-08-10 18:31:17  \n",
       "49        2011-03-03 11:40:25  \n",
       "50        2010-09-10 19:34:22  \n",
       "51        2011-04-05 14:00:24  \n",
       "52        2010-08-03 16:54:30  \n",
       "53        2010-09-29 16:23:30  \n",
       "54        2010-08-19 09:58:36  \n",
       "55        2010-09-09 20:45:13  \n",
       "56        2010-08-18 17:37:38  \n",
       "57        2010-12-09 16:05:04  \n",
       "58        2010-10-29 16:40:54  \n",
       "59        2010-10-06 16:05:32  \n",
       "60        2010-08-13 19:08:58  \n",
       "61        2010-10-01 19:48:03  \n",
       "62        2010-10-15 20:01:27  \n",
       "63        2010-06-11 17:42:48  \n",
       "64        2010-06-18 17:50:42  \n",
       "65        2010-07-16 19:33:29  \n",
       "66        2011-04-29 20:04:27  \n",
       "67        2010-07-23 18:01:20  \n",
       "68        2010-11-05 19:29:06  \n",
       "69        2010-12-10 17:39:36  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarfile_name = \"r4.2\"\n",
    "tarfile_url = \"C:\\\\Users\\\\son\\\\Downloads\\\\12841247\\\\\" + tarfile_name\n",
    "answer_root_url = \"C:\\\\Users\\\\son\\\\Downloads\\\\12841247\\\\answers\\\\\"\n",
    "answer_master_url = answer_root_url + \"insiders.csv\"\n",
    "answer_url = answer_root_url + tarfile_name\n",
    "answer =  answer_preprocess(answer_master_url,tarfile_name)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f181bc-5ec0-4c33-86c6-6d9843b87945",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbbc1c08e234d90976dd57620eb40aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f547edce3a047129625c294e95c3b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841c35386af64d5b8a431874f28f62cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14f8cde30db4406bf8dc24f7aee4712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_epochss = [2,3,4,5,6,7,8,9,10]\n",
    "min_slices = [5,10,15,20,30,60]\n",
    "\n",
    "# https_weight=[0]\n",
    "# connects_weight=[0]\n",
    "# mails_weight=[0]\n",
    "https_weight=list(map(lambda x: x*0.01, range(50)))\n",
    "connects_weight=list(map(lambda x: x*0.01, range(50)))\n",
    "mails_weight=list(map(lambda x: x*0.01, range(50)))\n",
    "weights=[[]]\n",
    "start_weights_index = 0\n",
    "\n",
    "for x in https_weight:\n",
    "    for y in connects_weight:\n",
    "        for z in mails_weight:\n",
    "            weights[start_weights_index].append(x)\n",
    "            weights[start_weights_index].append(y)\n",
    "            weights[start_weights_index].append(z)\n",
    "            weights.append([])\n",
    "            start_weights_index += 1\n",
    "weights = weights[:-1]\n",
    "\n",
    "# tarfile_names = ['r4.2']\n",
    "# tarfile_names = ['r5.2']\n",
    "# tarfile_names = ['r4.2','r5.2']\n",
    "# tarfile_names = ['r6.1','r6.2']\n",
    "tarfile_names = ['r4.2','r5.2','r6.1','r6.2']\n",
    "\n",
    "# 자세히보기 verbose 1은 그래프출력, 0은 출력안함\n",
    "verbose=0\n",
    "\n",
    "score_list =[[]]\n",
    "start_score_index = 0\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('device : ',device)\n",
    "\n",
    "# ================================================================================================\n",
    "# 1.기본 데이터 디렉터리 지정\n",
    "# ================================================================================================\n",
    "# Random_Seed 설정\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "for min_slice in tqdm(min_slices):\n",
    "    for tr_epochs in tqdm(tr_epochss):\n",
    "        \n",
    "        love=[[]]\n",
    "        losses_sum=[[]]\n",
    "        start_fuck_index = 0\n",
    "\n",
    "        norm_love=[[]]\n",
    "        norm_losses_sum=[[]]\n",
    "        norm_start_fuck_index = 0\n",
    "\n",
    "        test_love=[[]]\n",
    "        test_losses_sum=[[]]\n",
    "        test_start_fuck_index = 0\n",
    "        \n",
    "        user_counts = [[]]\n",
    "        user_counts_index = 0\n",
    "        \n",
    "        # Mal 행동 개수 측정\n",
    "        scenario_class=[0,0,0,0,0]\n",
    "        # Mal 행동 세션 개수 측정\n",
    "        scenario_session=[0,0,0,0,0]\n",
    "        # normal 행동 세션 개수 측정\n",
    "        scenario_norm_session=[0,0,0,0,0]\n",
    "        # test 행동 세션 개수 측정\n",
    "        scenario_test_session=[0,0,0,0,0]\n",
    "        \n",
    "        for tarfile_name in tarfile_names:\n",
    "            tarfile_url = \"C:\\\\Users\\\\son\\\\Downloads\\\\12841247\\\\\" + tarfile_name\n",
    "            answer_root_url = \"C:\\\\Users\\\\son\\\\Downloads\\\\12841247\\\\answers\\\\\"\n",
    "            answer_master_url = answer_root_url + \"insiders.csv\"\n",
    "            answer_url = answer_root_url + tarfile_name\n",
    "            token = make_token(min_slice)\n",
    "            real_tr_epochs = tr_epochs\n",
    "            # score_list[start_score_index].append(min_slice)\n",
    "            # score_list[start_score_index].append(tr_epochs)\n",
    "\n",
    "            # ================================================================================================\n",
    "            # 2.(전처리 1/2) 정상데이터, 비정상데이터 분리\n",
    "            # ================================================================================================\n",
    "            answer =  answer_preprocess(answer_master_url,tarfile_name)\n",
    "\n",
    "            if tarfile_name == \"r5.2\":\n",
    "                answer =  answer.loc[answer.scenario==4]\n",
    "                # answer =  answer.loc[answer.user=='GCD0194']\n",
    "                print('')\n",
    "            elif tarfile_name == \"r4.2\":\n",
    "                # answer =  answer.loc[answer.scenario==2]\n",
    "                # answer =  answer.loc[answer.user=='FSC0601']\n",
    "                print('')\n",
    "            elif tarfile_name == \"r6.1\":\n",
    "                answer =  answer.loc[answer.scenario==5]\n",
    "                # answer =  answer.loc[answer.user=='ADC1257']\n",
    "                print('')\n",
    "            elif tarfile_name == \"r6.2\":\n",
    "                answer =  answer.loc[answer.scenario==5]\n",
    "                # answer =  answer.loc[answer.user=='MBG3183']\n",
    "                print('') \n",
    "\n",
    "            for loveuser in answer['user'].unique():\n",
    "                # print('====================================')\n",
    "                # print(loveuser)\n",
    "                con_df = load_pickle('con_df', tarfile_url, tarfile_name)\n",
    "                con_df = con_df.loc[con_df.user==loveuser]\n",
    "\n",
    "                # ================================================================================================\n",
    "                # 3.(전처리 2/2) BERT 학습용 토큰 만들기\n",
    "                # ================================================================================================\n",
    "                # 정상 행위, 비정상 행위 토큰 생성하기\n",
    "                behavior, mal_behavior, norm_https, mal_https, norm_connects, mal_connects, norm_mails, mal_mails = behavior_convert_based_session2(con_df, answer, token, scenario_class, answer_url, min_slice, loveuser)\n",
    "                \n",
    "                user_counts[user_counts_index].append(norm_https)\n",
    "                user_counts[user_counts_index].append(mal_https)\n",
    "                user_counts[user_counts_index].append(norm_connects)\n",
    "                user_counts[user_counts_index].append(mal_connects)\n",
    "                user_counts[user_counts_index].append(norm_mails)\n",
    "                user_counts[user_counts_index].append(mal_mails)\n",
    "                user_counts_index += 1\n",
    "                user_counts.append([])\n",
    "                \n",
    "                behavior, test_behavior = train_test_split(behavior, test_size=0.2, shuffle=True, random_state=34)\n",
    "                behavior, val_behavior = train_test_split(behavior, test_size=0.1, shuffle=True, random_state=34)\n",
    "                \n",
    "                scenario_norm_session[int(answer.loc[answer.user==loveuser].scenario)-1] = scenario_norm_session[int(answer.loc[answer.user==loveuser].scenario)-1] + len(behavior)\n",
    "                scenario_session[int(answer.loc[answer.user==loveuser].scenario)-1] = scenario_session[int(answer.loc[answer.user==loveuser].scenario)-1] + len(mal_behavior)\n",
    "                scenario_test_session[int(answer.loc[answer.user==loveuser].scenario)-1] = scenario_test_session[int(answer.loc[answer.user==loveuser].scenario)-1] + len(test_behavior)\n",
    "                \n",
    "                save_pickle(behavior, 'behavior', tarfile_url, tarfile_name)\n",
    "                save_pickle(val_behavior, 'val_behavior', tarfile_url, tarfile_name)\n",
    "                save_pickle(mal_behavior, 'mal_behavior', tarfile_url, tarfile_name)\n",
    "                save_pickle(test_behavior, 'test_behavior', tarfile_url, tarfile_name)\n",
    "\n",
    "                # 만들어둔 정상 행위, 비정상 행위 토큰 불러오기\n",
    "                behavior = load_pickle('behavior', tarfile_url, tarfile_name)\n",
    "                mal_behavior = load_pickle('mal_behavior', tarfile_url, tarfile_name)\n",
    "                val_behavior = load_pickle('val_behavior', tarfile_url, tarfile_name)\n",
    "                test_behavior = load_pickle('test_behavior', tarfile_url, tarfile_name)\n",
    "\n",
    "                # 트레인용 정상행위 attention_mask, masking, lables 획득\n",
    "                con_df = pd.DataFrame(behavior)\n",
    "                con_df = con_df.fillna(0)\n",
    "                input_ids, attention_masks = make_behavior_token(con_df)\n",
    "                masked_input_ids, labels = create_masking(input_ids)\n",
    "\n",
    "                # 검증용 정상행위 attention_mask, masking, lables 획득\n",
    "                val_con_df = pd.DataFrame(val_behavior)\n",
    "                val_con_df = val_con_df.fillna(0)\n",
    "                val_input_ids, val_attention_masks = make_behavior_token(val_con_df)\n",
    "                val_masked_input_ids, val_labels = create_masking(val_input_ids)\n",
    "\n",
    "                # 비정상행위 attention_mask, masking, lables 획득\n",
    "                mal_con_df = pd.DataFrame(mal_behavior)\n",
    "                mal_con_df = mal_con_df.fillna(0)\n",
    "                mal_input_ids, mal_attention_masks = make_behavior_token(mal_con_df)\n",
    "                mal_masked_input_ids, mal_labels = create_masking(mal_input_ids)\n",
    "\n",
    "                # 테스트용 행위 attention_mask, masking, lables 획득\n",
    "                test_con_df = pd.DataFrame(test_behavior)\n",
    "                test_con_df = test_con_df.fillna(0)\n",
    "                test_input_ids, test_attention_masks = make_behavior_token(test_con_df)\n",
    "                test_masked_input_ids, test_labels = create_masking(test_input_ids)\n",
    "\n",
    "                # 생성된 데이터를 Pickle로 저장\n",
    "                # 학습용 정상데이터 저장\n",
    "                save_pickle(input_ids, 'input_ids', tarfile_url, tarfile_name)\n",
    "                save_pickle(attention_masks, 'attention_masks', tarfile_url, tarfile_name)\n",
    "                save_pickle(labels, 'labels', tarfile_url, tarfile_name)\n",
    "                save_pickle(masked_input_ids, 'masked_input_ids', tarfile_url, tarfile_name)\n",
    "\n",
    "                # 검증요 정상데이터 저장\n",
    "                save_pickle(val_input_ids, 'val_input_ids', tarfile_url, tarfile_name)\n",
    "                save_pickle(val_attention_masks, 'val_attention_masks', tarfile_url, tarfile_name)\n",
    "                save_pickle(val_labels, 'val_labels', tarfile_url, tarfile_name)\n",
    "                save_pickle(val_masked_input_ids, 'val_masked_input_ids', tarfile_url, tarfile_name)\n",
    "\n",
    "                # 비정상데이터 저장\n",
    "                save_pickle(mal_input_ids, 'mal_input_ids', tarfile_url, tarfile_name)\n",
    "                save_pickle(mal_attention_masks, 'mal_attention_masks', tarfile_url, tarfile_name)\n",
    "                save_pickle(mal_labels, 'mal_labels', tarfile_url, tarfile_name)\n",
    "                save_pickle(mal_masked_input_ids, 'mal_masked_input_ids', tarfile_url, tarfile_name)\n",
    "\n",
    "                # 테스트 데이터 저장\n",
    "                save_pickle(test_input_ids, 'test_input_ids', tarfile_url, tarfile_name)\n",
    "                save_pickle(test_attention_masks, 'test_attention_masks', tarfile_url, tarfile_name)\n",
    "                save_pickle(test_labels, 'test_labels', tarfile_url, tarfile_name)\n",
    "                save_pickle(test_masked_input_ids, 'test_masked_input_ids', tarfile_url, tarfile_name)\n",
    "\n",
    "                # 만들어둔 데이터를 Pickle로 불러오기\n",
    "                # 학습용 정상데이터 불러오기\n",
    "                input_ids = load_pickle('input_ids', tarfile_url, tarfile_name)\n",
    "                masked_input_ids = load_pickle('masked_input_ids', tarfile_url, tarfile_name)\n",
    "                attention_masks = load_pickle('attention_masks', tarfile_url, tarfile_name)\n",
    "                labels = load_pickle('labels', tarfile_url, tarfile_name)\n",
    "                token_type_ids = masked_input_ids * 0\n",
    "\n",
    "                # 검증용 정상데이터 불러오기\n",
    "                val_input_ids = load_pickle('val_input_ids', tarfile_url, tarfile_name)\n",
    "                val_masked_input_ids = load_pickle('val_masked_input_ids', tarfile_url, tarfile_name)\n",
    "                val_attention_masks = load_pickle('val_attention_masks', tarfile_url, tarfile_name)\n",
    "                val_labels = load_pickle('val_labels', tarfile_url, tarfile_name)\n",
    "                val_token_type_ids = mal_masked_input_ids * 0\n",
    "\n",
    "                # 비정상데이터 불러오기\n",
    "                mal_input_ids = load_pickle('mal_input_ids', tarfile_url, tarfile_name)\n",
    "                mal_masked_input_ids = load_pickle('mal_masked_input_ids', tarfile_url, tarfile_name)\n",
    "                mal_attention_masks = load_pickle('mal_attention_masks', tarfile_url, tarfile_name)\n",
    "                mal_labels = load_pickle('mal_labels', tarfile_url, tarfile_name)\n",
    "                mal_token_type_ids = mal_masked_input_ids * 0\n",
    "\n",
    "                # 테스트 데이터 불러오기\n",
    "                test_input_ids = load_pickle('test_input_ids', tarfile_url, tarfile_name)\n",
    "                test_masked_input_ids = load_pickle('test_masked_input_ids', tarfile_url, tarfile_name)\n",
    "                test_attention_masks = load_pickle('test_attention_masks', tarfile_url, tarfile_name)\n",
    "                test_labels = load_pickle('test_labels', tarfile_url, tarfile_name)\n",
    "                test_token_type_ids = test_masked_input_ids * 0\n",
    "\n",
    "                # ================================================================================================\n",
    "                # 6.Train용 Dataset설정, DataLoader 설정\n",
    "                # ================================================================================================\n",
    "                input_ids = load_pickle('input_ids', tarfile_url, tarfile_name)\n",
    "                masked_input_ids = load_pickle('masked_input_ids', tarfile_url, tarfile_name)\n",
    "                attention_masks = load_pickle('attention_masks', tarfile_url, tarfile_name)\n",
    "                labels = load_pickle('labels', tarfile_url, tarfile_name)\n",
    "                token_type_ids = masked_input_ids * 0\n",
    "\n",
    "                val_input_ids = load_pickle('val_input_ids', tarfile_url, tarfile_name)\n",
    "                val_masked_input_ids = load_pickle('val_masked_input_ids', tarfile_url, tarfile_name)\n",
    "                val_attention_masks = load_pickle('val_attention_masks', tarfile_url, tarfile_name)\n",
    "                val_labels = load_pickle('val_labels', tarfile_url, tarfile_name)\n",
    "                val_token_type_ids = mal_masked_input_ids * 0\n",
    "\n",
    "                train_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "                val_dataset =Val_Dataset(val_masked_input_ids, val_token_type_ids, val_attention_masks, val_labels)\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "                val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "\n",
    "                # ================================================================================================\n",
    "                # 7.BertMaskedLM 모델설정\n",
    "                # ================================================================================================\n",
    "                config = BertConfig(\n",
    "                    vocab_size=len(token)+1,\n",
    "                    max_position_embeddings=512,\n",
    "                    hidden_size=768,\n",
    "                    num_attention_heads=12,\n",
    "                    num_hidden_layers=6,\n",
    "                    pad_token_ids=0\n",
    "                )\n",
    "                model = BertForMaskedLM(config).to(device)\n",
    "\n",
    "                # ================================================================================================\n",
    "                # 8.옵티마이저 설정\n",
    "                # ================================================================================================\n",
    "                tr_optim = AdamW(model.parameters(), lr=1e-4 ,no_deprecation_warning=True)\n",
    "\n",
    "                # ================================================================================================\n",
    "                # 9.BERT MaskedLM 모델 사전학습 (custom pre-train)\n",
    "                # ================================================================================================\n",
    "                tr_losses=np.empty((0))\n",
    "                val_losses=np.empty((0))\n",
    "                output=[]\n",
    "                for tr_epoch in range(tr_epochs):\n",
    "\n",
    "                    # Training Phase\n",
    "                    loop = train_loader\n",
    "                    for x,batch in enumerate(loop):\n",
    "                        tr_optim.zero_grad()\n",
    "\n",
    "                        masked_input_id = batch[0].to(device)\n",
    "                        token_type_id = batch[1].to(device)\n",
    "                        attention_mask = batch[2].to(device)\n",
    "                        label = batch[3].to(device)\n",
    "\n",
    "                        outputs = model(masked_input_id, token_type_ids=token_type_id, attention_mask=attention_mask, labels=label, output_hidden_states=True)\n",
    "                        loss = outputs.loss\n",
    "                        loss.backward()\n",
    "                        tr_optim.step()\n",
    "                        tr_losses = np.append(tr_losses, loss.item())\n",
    "\n",
    "                    # Validation Phase\n",
    "                    loop = val_loader\n",
    "                    with torch.no_grad():\n",
    "                        for x,batch in enumerate(loop):\n",
    "\n",
    "                            val_masked_input_id = batch[0].to(device)\n",
    "                            val_token_type_id = batch[1].to(device)\n",
    "                            val_attention_mask = batch[2].to(device)\n",
    "                            val_label = batch[3].to(device)\n",
    "\n",
    "                            outputs = model(val_masked_input_id, token_type_ids=val_token_type_id, attention_mask=val_attention_mask, labels=val_label)\n",
    "                            loss = outputs.loss\n",
    "                            val_losses = np.append(val_losses, loss.item())\n",
    "\n",
    "                model.save_pretrained(tarfile_url + \"\\\\ITDBERT\")\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                \n",
    "#                 # 학습결과 시각화 한번 해봄\n",
    "#                 if verbose == 1:\n",
    "#                     ns.set(style='darkgrid')\n",
    "\n",
    "#                     sns.set(font_scale=1.5)\n",
    "#                     plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "\n",
    "#                     plt.plot(tr_losses, label='train_loss')\n",
    "#                     plt.plot(val_losses, label='validation_loss')\n",
    "\n",
    "#                     plt.title(\"Train & Validation loss\")\n",
    "#                     plt.xlabel(\"session_index\")\n",
    "#                     plt.xticks([x for x in range(len(tr_losses))])\n",
    "#                     plt.xticks(rotation = 90)\n",
    "#                     plt.ylabel(\"session_loss\")\n",
    "#                     plt.legend()\n",
    "#                     plt.show()\n",
    "        \n",
    "                # ================================================================================================\n",
    "                # MaskedLM 으로 이상치 탐지 - 정상\n",
    "                # ================================================================================================\n",
    "                input_ids = load_pickle('input_ids', tarfile_url, tarfile_name)\n",
    "                masked_input_ids = load_pickle('masked_input_ids', tarfile_url, tarfile_name)\n",
    "                attention_masks = load_pickle('attention_masks', tarfile_url, tarfile_name)\n",
    "                labels = load_pickle('labels', tarfile_url, tarfile_name)\n",
    "                token_type_ids = masked_input_ids * 0\n",
    "\n",
    "                train_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "                model = BertForMaskedLM.from_pretrained(tarfile_url + \"\\\\ITDBERT\").to(device)\n",
    "\n",
    "                norm_epochs = 1\n",
    "                losses=np.empty((0))\n",
    "                with torch.no_grad():\n",
    "                    for norm_epoch in range(norm_epochs):\n",
    "                        loop = train_loader\n",
    "                        for x,batch in enumerate(loop):\n",
    "\n",
    "                            masked_input_ids = batch[0].to(device)\n",
    "                            token_type_ids = batch[1].to(device)\n",
    "                            attention_masks = batch[2].to(device)\n",
    "                            labels = batch[3].to(device)\n",
    "\n",
    "                            outputs = model(masked_input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "                            loss = outputs.loss\n",
    "                            losses = np.append(losses, loss.cpu())\n",
    "                \n",
    "                cut_off = losses.max()\n",
    "                cut_off_line = [cut_off] * len(losses)\n",
    "                norm_loss_fu = losses.tolist()\n",
    "                for x in norm_loss_fu:\n",
    "                    norm_losses_sum[norm_start_fuck_index].append(x)\n",
    "\n",
    "                norm_love[norm_start_fuck_index].append(loveuser)\n",
    "                norm_love[norm_start_fuck_index].append(cut_off)\n",
    "                norm_love.append([])\n",
    "                norm_losses_sum.append([])\n",
    "                norm_start_fuck_index += 1\n",
    "\n",
    "                # if verbose == 1:\n",
    "                #     # print('https 표준편차')\n",
    "                #     # print(std_norm_https)\n",
    "                #     # print('')\n",
    "                #     # print('connects 표준편차')\n",
    "                #     # print(std_norm_connects)\n",
    "                #     # print('')\n",
    "                #     # print('mails 표준편차')\n",
    "                #     # print(std_norm_mails)\n",
    "                #     sns.set(font_scale=1.5)\n",
    "                #     plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "                #     plt.plot(losses, label='loss')\n",
    "                #     plt.title(\"Normal_session\")\n",
    "                #     plt.xlabel(\"session_index\")\n",
    "                #     plt.plot(cut_off_line, 'r-', label = 'cut_off')\n",
    "                #     plt.xticks([x for x in range(len(losses))])\n",
    "                #     plt.xticks(rotation = 90)\n",
    "                #     plt.ylabel(\"session_loss\")\n",
    "                #     plt.legend()\n",
    "                #     plt.show()\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # ================================================================================================\n",
    "                # MaskedLM 으로 이상치 탐지 - 테스트\n",
    "                # ================================================================================================\n",
    "                input_ids = load_pickle('test_input_ids', tarfile_url, tarfile_name)\n",
    "                masked_input_ids = load_pickle('test_masked_input_ids', tarfile_url, tarfile_name)\n",
    "                attention_masks = load_pickle('test_attention_masks', tarfile_url, tarfile_name)\n",
    "                labels = load_pickle('test_labels', tarfile_url, tarfile_name)\n",
    "                token_type_ids = masked_input_ids * 0\n",
    "\n",
    "                train_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "                model = BertForMaskedLM.from_pretrained(tarfile_url + \"\\\\ITDBERT\").to(device)\n",
    "\n",
    "                test_epochs = 1\n",
    "                losses=np.empty((0))\n",
    "                with torch.no_grad():\n",
    "                    for test_epoch in range(test_epochs):\n",
    "                        loop = train_loader\n",
    "                        for x,batch in enumerate(loop):\n",
    "\n",
    "                            masked_input_ids = batch[0].to(device)\n",
    "                            token_type_ids = batch[1].to(device)\n",
    "                            attention_masks = batch[2].to(device)\n",
    "                            labels = batch[3].to(device)\n",
    "\n",
    "                            outputs = model(masked_input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "                            loss = outputs.loss\n",
    "                            losses = np.append(losses, loss.cpu())\n",
    "                \n",
    "                test_loss_fu = losses.tolist()\n",
    "                for x in test_loss_fu:\n",
    "                    test_losses_sum[test_start_fuck_index].append(x)\n",
    "\n",
    "                test_love[start_fuck_index].append(loveuser)\n",
    "                test_love[start_fuck_index].append(cut_off)\n",
    "                test_love.append([])\n",
    "                test_losses_sum.append([])\n",
    "                test_start_fuck_index += 1\n",
    "\n",
    "#                 # 세션나온거 시각화 한번 해봄\n",
    "#                 if verbose == 1:\n",
    "#                     sns.set(style='darkgrid')\n",
    "\n",
    "#                     sns.set(font_scale=1.5)\n",
    "#                     plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "\n",
    "#                     plt.plot(losses, label='losses')\n",
    "#                     cut_off_line = [cut_off] * len(losses)\n",
    "#                     plt.plot(cut_off_line, 'r-', label = 'cut_off')\n",
    "\n",
    "#                     plt.title(\"Test_session\")\n",
    "#                     plt.xlabel(\"session_index\")\n",
    "#                     plt.ylabel(\"session_loss\")\n",
    "#                     plt.xticks([x for x in range(len(losses))])\n",
    "#                     plt.xticks(rotation = 90)\n",
    "#                     plt.legend()\n",
    "#                     plt.show()\n",
    "\n",
    "                    # print('https 표준편차')\n",
    "                    # print(std_mal_https)\n",
    "                    # print('')\n",
    "                    # print('connects 표준편차')\n",
    "                    # print(std_mal_connects)\n",
    "                    # print('')\n",
    "                    # print('mails 표준편차')\n",
    "                    # print(std_mal_mails)\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # ================================================================================================\n",
    "                # MaskedLM 으로 이상치 탐지 - 비정상\n",
    "                # ================================================================================================\n",
    "                input_ids = load_pickle('mal_input_ids', tarfile_url, tarfile_name)\n",
    "                masked_input_ids = load_pickle('mal_masked_input_ids', tarfile_url, tarfile_name)\n",
    "                attention_masks = load_pickle('mal_attention_masks', tarfile_url, tarfile_name)\n",
    "                labels = load_pickle('mal_labels', tarfile_url, tarfile_name)\n",
    "                token_type_ids = masked_input_ids * 0\n",
    "\n",
    "                train_dataset =Train_Dataset(masked_input_ids, token_type_ids, attention_masks, labels)\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1)\n",
    "\n",
    "                model = BertForMaskedLM.from_pretrained(tarfile_url + \"\\\\ITDBERT\").to(device)\n",
    "\n",
    "                abnormal_epochs = 1\n",
    "                losses=np.empty((0))\n",
    "                with torch.no_grad():\n",
    "                    for abnormal_epoch in range(abnormal_epochs):\n",
    "                        loop = train_loader\n",
    "                        for x,batch in enumerate(loop):\n",
    "\n",
    "                            masked_input_ids = batch[0].to(device)\n",
    "                            token_type_ids = batch[1].to(device)\n",
    "                            attention_masks = batch[2].to(device)\n",
    "                            labels = batch[3].to(device)\n",
    "\n",
    "                            outputs = model(masked_input_ids, token_type_ids=token_type_ids, attention_mask=attention_masks, labels=labels)\n",
    "                            loss = outputs.loss\n",
    "                            losses = np.append(losses, loss.cpu())                \n",
    "\n",
    "                loss_fu = losses.tolist()\n",
    "\n",
    "                for x in loss_fu:\n",
    "                    losses_sum[start_fuck_index].append(x)\n",
    "\n",
    "                love[start_fuck_index].append(loveuser)\n",
    "                love[start_fuck_index].append(cut_off)\n",
    "                love.append([])\n",
    "                losses_sum.append([])\n",
    "                start_fuck_index += 1\n",
    "                \n",
    "                # 세션나온거 시각화 한번 해봄\n",
    "#                 if verbose == 1:\n",
    "#                     sns.set(style='darkgrid')\n",
    "\n",
    "#                     sns.set(font_scale=1.5)\n",
    "#                     plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "\n",
    "#                     plt.plot(losses, label='losses')\n",
    "#                     cut_off_line = [cut_off] * len(losses)\n",
    "#                     plt.plot(cut_off_line, 'r-', label = 'cut_off')\n",
    "\n",
    "#                     plt.title(\"Abnormal_session\")\n",
    "#                     plt.xlabel(\"session_index\")\n",
    "#                     plt.ylabel(\"session_loss\")\n",
    "\n",
    "#                     plt.xticks([x for x in range(len(losses))])\n",
    "#                     plt.xticks(rotation = 90)\n",
    "#                     plt.legend()\n",
    "#                     plt.show()\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        love = love[:-1]\n",
    "        losses_sum = losses_sum[:-1]\n",
    "\n",
    "        norm_love = norm_love[:-1]\n",
    "        norm_losses_sum = norm_losses_sum[:-1]\n",
    "\n",
    "        test_love = test_love[:-1]\n",
    "        test_losses_sum = test_losses_sum[:-1]\n",
    "        \n",
    "        user_counts = user_counts[:-1]\n",
    "        \n",
    "        norm_losses_sum_bak = copy.deepcopy(norm_losses_sum)\n",
    "        test_losses_sum_bak = copy.deepcopy(test_losses_sum)\n",
    "        losses_sum_bak = copy.deepcopy(losses_sum)\n",
    "        \n",
    "        \n",
    "        # 0 : user_counts[user_counts_index].append(norm_https)\n",
    "        # 1 : user_counts[user_counts_index].append(mal_https)\n",
    "        # 2 : user_counts[user_counts_index].append(norm_connects)\n",
    "        # 3 : user_counts[user_counts_index].append(mal_connects)\n",
    "        # 4 : user_counts[user_counts_index].append(norm_mails)\n",
    "        # 5 : user_counts[user_counts_index].append(mal_mails)\n",
    "\n",
    "        for weight in tqdm(weights):    \n",
    "            scenario_thres_session=[0,0,0,0,0]\n",
    "            test_scenario_thres_session=[0,0,0,0,0]\n",
    "\n",
    "            for x, user_count in enumerate(user_counts):  \n",
    "                std_mal_https, std_norm_https = calc_standardize(user_count[1], user_count[0], weight[0])\n",
    "                std_mal_connects, std_norm_connects = calc_standardize(user_count[3], user_count[2], weight[1])\n",
    "                std_mal_mails, std_norm_mails = calc_standardize(user_count[5], user_count[4], weight[2])\n",
    "\n",
    "                std_norm_https, std_test_norm_https = train_test_split(std_norm_https, test_size=0.2, shuffle=True, random_state=34)\n",
    "                std_norm_https, std_val_norm_https = train_test_split(std_norm_https, test_size=0.1, shuffle=True, random_state=34)\n",
    "\n",
    "                std_norm_connects, std_test_norm_connects = train_test_split(std_norm_connects, test_size=0.2, shuffle=True, random_state=34)\n",
    "                std_norm_connects, std_val_norm_connects = train_test_split(std_norm_connects, test_size=0.1, shuffle=True, random_state=34)\n",
    "\n",
    "                std_norm_mails, std_test_norm_mails = train_test_split(std_norm_mails, test_size=0.2, shuffle=True, random_state=34)\n",
    "                std_norm_mails, std_val_norm_mails = train_test_split(std_norm_mails, test_size=0.1, shuffle=True, random_state=34)\n",
    "\n",
    "                std_norm_https = np.array(std_norm_https)\n",
    "                std_norm_https[np.isnan(std_norm_https)] = 0\n",
    "\n",
    "                std_test_norm_https = np.array(std_test_norm_https)\n",
    "                std_test_norm_https[np.isnan(std_test_norm_https)] = 0\n",
    "\n",
    "                std_val_norm_https = np.array(std_val_norm_https)\n",
    "                std_val_norm_https[np.isnan(std_val_norm_https)] = 0\n",
    "\n",
    "                std_mal_https = np.array(std_mal_https)\n",
    "                std_mal_https[np.isnan(std_mal_https)] = 0\n",
    "\n",
    "                std_norm_connects = np.array(std_norm_connects)\n",
    "                std_norm_connects[np.isnan(std_norm_connects)] = 0\n",
    "\n",
    "                std_test_norm_connects = np.array(std_test_norm_connects)\n",
    "                std_test_norm_connects[np.isnan(std_test_norm_connects)] = 0\n",
    "\n",
    "                std_val_norm_connects = np.array(std_val_norm_connects)\n",
    "                std_val_norm_connects[np.isnan(std_val_norm_connects)] = 0\n",
    "\n",
    "                std_mal_connects = np.array(std_mal_connects)\n",
    "                std_mal_connects[np.isnan(std_mal_connects)] = 0\n",
    "\n",
    "                std_norm_mails = np.array(std_norm_mails)\n",
    "                std_norm_mails[np.isnan(std_norm_mails)] = 0\n",
    "\n",
    "                std_test_norm_mails = np.array(std_test_norm_mails)\n",
    "                std_test_norm_mails[np.isnan(std_test_norm_mails)] = 0\n",
    "\n",
    "                std_val_norm_mails = np.array(std_val_norm_mails)\n",
    "                std_val_norm_mails[np.isnan(std_val_norm_mails)] = 0\n",
    "\n",
    "                std_mal_mails = np.array(std_mal_mails)\n",
    "                std_mal_mails[np.isnan(std_mal_mails)] = 0\n",
    "\n",
    "                #========== weight에 따른 cutoff 갱신=======================\n",
    "                norm_losses_sum[x] = copy.deepcopy(norm_losses_sum_bak[x])\n",
    "                test_losses_sum[x] = copy.deepcopy(test_losses_sum_bak[x])\n",
    "                losses_sum[x] = copy.deepcopy(losses_sum_bak[x])\n",
    "\n",
    "                # print('===========================')\n",
    "                # print(love[x][0])\n",
    "\n",
    "        #         print('norm_losses_sum : ',len(norm_losses_sum[x]))\n",
    "        #         print(norm_losses_sum[x])\n",
    "        #         print('norm_https : ',len(std_norm_https))\n",
    "        #         print(std_norm_https)\n",
    "        #         print('norm_connects : ',len(std_norm_connects))\n",
    "        #         print(std_norm_connects)\n",
    "        #         print('norm_mails : ',len(std_norm_mails))\n",
    "        #         print(std_norm_mails)\n",
    "        #         print('')\n",
    "\n",
    "        #         print('test_losses_sum : ',len(test_losses_sum[x]))\n",
    "        #         print(test_losses_sum[x])\n",
    "        #         print('test_https : ',len(std_test_norm_https))\n",
    "        #         print(std_test_norm_https)\n",
    "        #         print('test_connects : ',len(std_test_norm_connects))\n",
    "        #         print(std_test_norm_connects)\n",
    "        #         print('test_mails : ',len(std_test_norm_mails))\n",
    "        #         print(std_test_norm_mails)\n",
    "        #         print('')\n",
    "\n",
    "        #         print('mal_losses_sum : ',len(losses_sum[x]))\n",
    "        #         print(losses_sum[x])\n",
    "        #         print('mal_https : ',len(std_mal_https))\n",
    "        #         print(std_mal_https)\n",
    "        #         print('mal_connects : ',len(std_mal_connects))\n",
    "        #         print(std_mal_connects)\n",
    "        #         print('mal_mails : ',len(std_mal_mails))\n",
    "        #         print(std_mal_mails)\n",
    "        #         print('')\n",
    "\n",
    "                #print('최초 임계치 : ', love[x][1])\n",
    "\n",
    "                for y in range(len(norm_losses_sum[x])):\n",
    "                    norm_losses_sum[x][y] = norm_losses_sum[x][y] + std_norm_https[y] + std_norm_connects[y] + std_norm_mails[y]\n",
    "\n",
    "                for y in range(len(test_losses_sum[x])):\n",
    "                    test_losses_sum[x][y] = test_losses_sum[x][y] +  std_test_norm_https[y] +  std_test_norm_connects[y] + std_test_norm_mails[y]\n",
    "\n",
    "                for y in range(len(losses_sum[x])):\n",
    "                    losses_sum[x][y] = losses_sum[x][y] + std_mal_https[y] +  std_mal_connects[y] + std_mal_mails[y]\n",
    "\n",
    "                threshold_imsi = 0\n",
    "                for y in norm_losses_sum[x]:\n",
    "                    if y > threshold_imsi:\n",
    "                        threshold_imsi = y\n",
    "                love[x][1] = threshold_imsi\n",
    "                test_love[x][1] = threshold_imsi\n",
    "\n",
    "                #print('바뀐 임계치 : ', love[x][1])\n",
    "\n",
    "                if verbose == 1:\n",
    "                    cut_off_line = [love[x][1]] * len(norm_losses_sum[x])\n",
    "                    sns.set(style='darkgrid')\n",
    "                    sns.set(font_scale=1.5)\n",
    "                    plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "                    plt.plot(norm_losses_sum[x], label='loss')\n",
    "                    plt.title(\"Normal_session\")\n",
    "                    plt.xlabel(\"session_index\")\n",
    "                    plt.plot(cut_off_line, 'r-', label = 'cut_off')\n",
    "                    plt.xticks([x for x in range(len(norm_losses_sum[x]))])\n",
    "                    plt.xticks(rotation = 90)\n",
    "                    plt.ylabel(\"session_loss\")\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                    sns.set(style='darkgrid')\n",
    "                    sns.set(font_scale=1.5)\n",
    "                    plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "                    plt.plot(test_losses_sum[x], label='losses')\n",
    "                    cut_off_line = [love[x][1]] * len(test_losses_sum[x])\n",
    "                    plt.plot(cut_off_line, 'r-', label = 'cut_off')\n",
    "                    plt.title(\"Test_session\")\n",
    "                    plt.xlabel(\"session_index\")\n",
    "                    plt.ylabel(\"session_loss\")\n",
    "                    plt.xticks([x for x in range(len(test_losses_sum[x]))])\n",
    "                    plt.xticks(rotation = 90)\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                    sns.set(style='darkgrid')\n",
    "                    sns.set(font_scale=1.5)\n",
    "                    plt.rcParams[\"figure.figsize\"] = (60,10)\n",
    "                    plt.plot(losses_sum[x], label='losses')\n",
    "                    cut_off_line = [love[x][1]] * len(losses_sum[x])\n",
    "                    plt.plot(cut_off_line, 'r-', label = 'cut_off')\n",
    "                    plt.title(\"Abnormal_session\")\n",
    "                    plt.xlabel(\"session_index\")\n",
    "                    plt.ylabel(\"session_loss\")\n",
    "                    plt.xticks([x for x in range(len(losses_sum[x]))])\n",
    "                    plt.xticks(rotation = 90)\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "            #=================================================================================================================================================================================================================\n",
    "            all_sum = 0\n",
    "            under_cutoff =0\n",
    "            under_founds = [[]]\n",
    "            under_index = 0\n",
    "\n",
    "            for x in range(len(love)):\n",
    "                under_found = 0\n",
    "                all_sum += len(losses_sum[x])\n",
    "                for y in range(len(losses_sum[x])):\n",
    "                    if losses_sum[x][y] < love[x][1]:\n",
    "                        under_found += 1\n",
    "                if under_found >= 1:\n",
    "                    try:\n",
    "                        answer =  answer_preprocess(answer_master_url,'r4.2')\n",
    "                        scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + under_found\n",
    "                    except:\n",
    "                        try:\n",
    "                            answer =  answer_preprocess(answer_master_url,'r5.2')\n",
    "                            scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + under_found\n",
    "                        except:\n",
    "                            try:\n",
    "                                answer =  answer_preprocess(answer_master_url,'r6.1')\n",
    "                                scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + under_found\n",
    "                            except:\n",
    "                                answer =  answer_preprocess(answer_master_url,'r6.2')\n",
    "                                scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + under_found\n",
    "\n",
    "                    under_founds[under_index].append(love[x][0])\n",
    "                    under_founds[under_index].append(under_found)\n",
    "                    under_founds.append([])\n",
    "                    under_index +=1\n",
    "\n",
    "            under_founds = under_founds[:-1]\n",
    "            for x in range(len(under_founds)):\n",
    "                under_cutoff += under_founds[x][1]\n",
    "\n",
    "            #=================================================================================================================================================================================================================    \n",
    "            test_all_sum = 0\n",
    "            test_under_cutoff =0\n",
    "            test_under_founds = [[]]\n",
    "            test_under_index = 0\n",
    "\n",
    "            for x in range(len(test_love)):\n",
    "                test_under_found = 0\n",
    "                test_all_sum += len(test_losses_sum[x])\n",
    "                for y in range(len(test_losses_sum[x])):\n",
    "                    if test_losses_sum[x][y] > test_love[x][1]:\n",
    "                        test_under_found += 1\n",
    "                if test_under_found >= 1:\n",
    "\n",
    "                    try:\n",
    "                        answer =  answer_preprocess(answer_master_url,'r4.2')\n",
    "                        test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + test_under_found\n",
    "                    except:\n",
    "                        try:\n",
    "                            answer =  answer_preprocess(answer_master_url,'r5.2')\n",
    "                            test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + test_under_found\n",
    "                        except:\n",
    "                            try:\n",
    "                                answer =  answer_preprocess(answer_master_url,'r6.1')\n",
    "                                test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + test_under_found\n",
    "                            except:\n",
    "                                answer =  answer_preprocess(answer_master_url,'r6.2')\n",
    "                                test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] = test_scenario_thres_session[int(answer.loc[answer.user==love[x][0]].scenario)-1] + test_under_found\n",
    "\n",
    "                    test_under_founds[test_under_index].append(test_love[x][0])\n",
    "                    test_under_founds[test_under_index].append(test_under_found)\n",
    "                    test_under_founds.append([])\n",
    "                    test_under_index +=1\n",
    "\n",
    "            test_under_founds = test_under_founds[:-1]\n",
    "            for x in range(len(test_under_founds)):\n",
    "                test_under_cutoff += test_under_founds[x][1]\n",
    "\n",
    "            #=================================================================================================================================================================================================================    \n",
    "        #     print('=========================== 정보 =======================')\n",
    "\n",
    "        #     print('')\n",
    "        #     print('tr_epochs : ',real_tr_epochs,  'min_slice : ', min_slice, 'http weight : ',weight[0], 'connect weight : ',weight[1], 'mail weight : ',weight[2])\n",
    "        #     print('')\n",
    "\n",
    "        #     for x in range(len(scenario_class)):\n",
    "        #         print(' 시나리오 ', x+1, '비정상 행동 수 :',scenario_class[x])\n",
    "        #     print(' ')\n",
    "        #     for x in range(len(scenario_session)):\n",
    "        #         print(' 시나리오 ', x+1, '정상 세션 수 :',scenario_norm_session[x])\n",
    "        #     print(' ')\n",
    "        #     for x in range(len(scenario_session)):\n",
    "        #         print(' 시나리오 ', x+1, '비정상 세션 수 :',scenario_session[x])\n",
    "        #     print(' ')\n",
    "        #     for x in range(len(scenario_session)):\n",
    "        #         print(' 시나리오 ', x+1, '비정상 정탐 수 :',scenario_session[x]-scenario_thres_session[x])    \n",
    "        #     print(' ')\n",
    "        #     for x in range(len(scenario_session)):\n",
    "        #         print(' 시나리오 ', x+1, '비정상 미탐 수 :',scenario_thres_session[x])\n",
    "        #     print(' ')\n",
    "        #     for x in range(len(scenario_session)):\n",
    "        #         print(' 시나리오 ', x+1, '테스트 세션 수 :',scenario_test_session[x])\n",
    "        #     print(' ')\n",
    "        #     for x in range(len(scenario_session)):\n",
    "        #         print(' 시나리오 ', x+1, '테스트 오탐 수 :',test_scenario_thres_session[x])\n",
    "        #     print(' ')\n",
    "\n",
    "        #     print('all_mal_count : ',all_sum)\n",
    "        #     print('under_cutoff_count : ',under_cutoff)\n",
    "        #     print('under_found list\\n', under_founds)\n",
    "\n",
    "        #     print(' ')\n",
    "        #     print('all_test_count : ',test_all_sum)\n",
    "        #     print('test_upper_cutoff_count : ',test_under_cutoff)\n",
    "        #     print('test_upper_found list\\n', test_under_founds)\n",
    "\n",
    "            # Precision, Recall, F1 score\n",
    "            try:\n",
    "                avg_precision = ((all_sum - under_cutoff)/(all_sum - under_cutoff + test_under_cutoff) + (test_all_sum - test_under_cutoff)/(test_all_sum - test_under_cutoff + under_cutoff))/2\n",
    "            except:\n",
    "                avg_precision = 0\n",
    "                print('zero found')\n",
    "\n",
    "            avg_recall = ((all_sum - under_cutoff)/(all_sum - under_cutoff + under_cutoff) + (test_all_sum - test_under_cutoff)/(test_all_sum - test_under_cutoff + test_under_cutoff))/2\n",
    "            f1_score = 2*((avg_precision * avg_recall)/(avg_precision + avg_recall))\n",
    "            accuracy = ((all_sum - under_cutoff) + (test_all_sum - test_under_cutoff))/(all_sum + test_all_sum)\n",
    "\n",
    "            #시나리오별 정, 오탐\n",
    "           # for x in range(len(scenario_session)):\n",
    "                #try:\n",
    "                    #print(' 시나리오 ', x+1, '정탐율 :',(scenario_session[x] - scenario_thres_session[x])/scenario_session[x] *100,\"%\")\n",
    "                    #print(' 시나리오 ', x+1, '오탐율 :',test_scenario_thres_session[x]/scenario_test_session[x] *100,\"%\")\n",
    "                    #print('')\n",
    "               # except:\n",
    "                    #print('zero divide')\n",
    "\n",
    "           # print('')\n",
    "          ##  print('종합 정탐율 : ',(all_sum - under_cutoff)/all_sum*100,\"%\")\n",
    "            #print('종합 오탐율 : ',test_under_cutoff/test_all_sum*100,\"%\")\n",
    "           # print('accuracy : ', accuracy)\n",
    "          #  print('precision : ', avg_precision)\n",
    "           # print('recall : ',avg_recall)\n",
    "            #print('f1_score : ',f1_score)\n",
    "\n",
    "            score_list[start_score_index].append(min_slice)\n",
    "            score_list[start_score_index].append(tr_epochs)\n",
    "            score_list[start_score_index].append(avg_precision)\n",
    "            score_list[start_score_index].append(avg_recall)\n",
    "            score_list[start_score_index].append(f1_score)\n",
    "            score_list[start_score_index].append((all_sum - under_cutoff)/all_sum*100)\n",
    "            score_list[start_score_index].append(test_under_cutoff/test_all_sum*100)\n",
    "            score_list[start_score_index].append(accuracy)\n",
    "            score_list[start_score_index].append(weight) # (구)weight\n",
    "\n",
    "            for x in range(len(scenario_session)):\n",
    "                try:\n",
    "                    score_list[start_score_index].append((scenario_session[x] - scenario_thres_session[x])/scenario_session[x] *100)\n",
    "                    score_list[start_score_index].append(test_scenario_thres_session[x]/scenario_test_session[x] *100)\n",
    "                except:\n",
    "                    score_list[start_score_index].append(0)\n",
    "                    score_list[start_score_index].append(0)\n",
    "\n",
    "            for x in range(len(scenario_session)):\n",
    "                score_list[start_score_index].append(str(scenario_session[x]-scenario_thres_session[x]) + \"/\" + str(scenario_session[x]))\n",
    "\n",
    "            score_list.append([])\n",
    "            start_score_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d202d79-e200-4961-81c0-a7a4d66464b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_explotiation(score_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
